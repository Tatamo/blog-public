<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Parser on わたしろぐ</title>
    <link>http://tatamo.81.la/blog/tags/parser/</link>
    <description>Recent content in Parser on わたしろぐ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 11 Feb 2017 15:13:48 +0900</lastBuildDate>
    <atom:link href="http://tatamo.81.la/blog/tags/parser/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第2回:字句解析器の実装</title>
      <link>http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/</link>
      <pubDate>Sat, 11 Feb 2017 15:13:48 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回&lt;/a&gt;は構文解析の概略を紹介したので、今回から実装に移っていきたいと思います。
まずは字句解析器を用意する必要があるため、今回は字句解析器の作成について紹介します。&lt;/p&gt;

&lt;p&gt;なお今回から実際のプログラムを記述していきますが、使用言語はTypeScriptとします。&lt;/p&gt;

&lt;p&gt;パーサジェネレータを作るのに比べれば字句解析器を作るのは非常に単純です。
早速はじめていきましょう。&lt;/p&gt;

&lt;h2 id=&#34;字句解析器の仕様を確認する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句解析器の仕様を確認する&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回&lt;/a&gt;の記事でも紹介しましたが、字句解析器の行う処理は以下のような流れになります。&lt;/p&gt;

&lt;p&gt;まず、解析するべき入力を文字列として受け取ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;9 + 11 * (2 + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これに加えて、字句規則を用意します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: /[1-9][0-9]*/
プラス: &amp;quot;+&amp;quot;
アステリスク: &amp;quot;*&amp;quot;
左括弧: &amp;quot;(&amp;quot;
右括弧: &amp;quot;)&amp;quot;
(読み捨て): /\s/
(不正): /./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字句解析器は受け取った入力を先頭から順に字句規則にあてはめ、マッチするものがあればそのトークンを割り当てます。
結果として得られる出力は、以下のようなリストになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: 9
プラス: +
数字: 11
アステリスク: *
左括弧: (
数字: 2
プラス: +
数字: 1
右括弧: )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;得られたトークンのリストを構文解析器の入力として渡すことで、構文解析器は文法の解析のみに注力することができます。&lt;/p&gt;

&lt;h2 id=&#34;字句規則を定義する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句規則を定義する&lt;/h2&gt;

&lt;p&gt;実際に解析を行うタイミングでは文字列のみを入力として受け取りますが、字句解析器の生成時には字句規則が必要です。
そのため、予め字句規則を別の設定ファイルなどに書いておくなどして用意しておかなければなりません。
ただし、&lt;strong&gt;字句規則の解析には構文解析器が必要&lt;/strong&gt;となるため、現時点ではプログラム内にハードコーディングしておくなどする必要があります。
今回は、字句規則を内部的に以下のようなデータ構造で扱うこととして、しばらくは字句規則をその内部データの形式で直接書くことにします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;// 定義
export type Token = string|symbol;
export interface LexDefinitionSection{
	token: Token|null;
	pattern: string|RegExp;
}
export type LexDefinitions = Array&amp;lt;LexDefinitionSection&amp;gt;;

// 実際の字句規則
const lex: LexDefinitions = [
	{token:&amp;quot;NUMBER&amp;quot;, pattern:/[1-9][0-9]*/},
	{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, pattern:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, pattern:&amp;quot;(&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, pattern:&amp;quot;)&amp;quot;},
	{token:null, pattern:/\s/},
	{token:&amp;quot;INVALID&amp;quot;, pattern:/./},
];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この字句規則の定義について、実用上の理由で追加したいくつかの仕様に注意する必要があります。&lt;/p&gt;

&lt;p&gt;Tokenの型定義にsymbolを含めている点についてはここで説明せずに後述することとします。&lt;/p&gt;

&lt;h3 id=&#34;入力の読み捨て:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;入力の読み捨て&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;{token:null, pattern:/\s/}
// (読み捨て): /\s/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この部分は、何らかの空白文字が入力に存在していればマッチングされます。
&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; を解析する際、ここに含まれている空白は文法上何の意味も持たず、&lt;code&gt;9+11*(2+1)&lt;/code&gt; のように入力が与えられたとしても解析結果は変化しません。
このような場合、構文解析器に空白の情報を与えることすらせずに、字句解析器上で空白を検知した段階でその情報を捨ててしまったほうが、構文解析器に余計な処理をさせずに済みます。&lt;/p&gt;

&lt;p&gt;今回は、トークンのラベル部分にnullを指定することで、読み取った結果をトークンとして保持することなく読み捨てることを表すようにしています。&lt;/p&gt;

&lt;h3 id=&#34;正規表現パターンと文字列パターンの使い分け:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;正規表現パターンと文字列パターンの使い分け&lt;/h3&gt;

&lt;p&gt;(ごちゃごちゃ書いている割に小手先のテクニックという感じが強いため、よくわからなければ読み飛ばしてください)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;}&lt;/code&gt; のように、パターン部分に正規表現ではなく文字列を用いて記述している箇所があります。
すべて正規表現を使って記述するのではなく文字列も許容している理由として、まず&lt;code&gt;/\+/&lt;/code&gt;のように特殊記号をエスケープせずに済む点が挙げられます。
そして、「文字列でパターンを記述した場合は、アルファベットの途中でトークンを区切らないようにする」というルールを用いることで、一部のパターンを簡潔に書くことが可能になります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vwxyz&lt;/code&gt; という入力を考えてみましょう。
ここでもし、&lt;code&gt;&amp;quot;vwx&amp;quot;&lt;/code&gt; にマッチする規則と、&lt;code&gt;&amp;quot;vwxyz&amp;quot;&lt;/code&gt; にマッチする規則の2つが存在した場合、&lt;code&gt;&amp;quot;vwxyz&amp;quot;&lt;/code&gt; の規則を先に書かない限り、入力&lt;code&gt;vwxyz&lt;/code&gt; は&lt;code&gt;&amp;quot;vwx&amp;quot; + yz&lt;/code&gt; とみなされ、&lt;code&gt;&amp;quot;yz&amp;quot;&lt;/code&gt; に対応する規則が存在しなければエラーとなります。
これを回避するためには、よりマッチするパターンが長い規則を常に短い規則よりも先に書くようにする必要がありますが、面倒です。
そこで、正規表現ではなく文字列で&lt;code&gt;&amp;quot;vwx&amp;quot;&lt;/code&gt; などのパターンが定義され、かつその末尾の文字が&lt;code&gt;\w&lt;/code&gt; にマッチする場合、マッチした部分の一文字先の文字が&lt;code&gt;\w&lt;/code&gt; 以外でなければマッチしないようにします。
これは、正規表現で&lt;code&gt;/vwx(?!\w)/&lt;/code&gt; 、&lt;code&gt;/vwxyz(?!\w)/&lt;/code&gt; というような否定的前方先読みをパターンの最後に追加することに相当します。
このルールを追加することで、正規表現を用いる場合よりも簡潔に記述可能となります。&lt;/p&gt;

&lt;h2 id=&#34;字句解析器を実装する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句解析器を実装する&lt;/h2&gt;

&lt;p&gt;ではLexerクラスを作っていきましょう。
とはいえ字句規則さえ定義してしまえば、やることはほとんどありません。
コンストラクタ引数として字句規則データを受け取って保持しておくようにして、解析実行時に上から順に字句規則のマッチングを試みるだけです。&lt;/p&gt;

&lt;p&gt;今回はコード量が少ないので、 &lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/lexer.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/lexer.ts&lt;/a&gt; 全体をそのまま貼り付けます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// token.ts
export type Token = string|symbol;
export const SYMBOL_EOF:Token = Symbol(&amp;quot;EOF&amp;quot;);
export const SYMBOL_SYNTAX:Token = Symbol(&amp;quot;S&#39;&amp;quot;);
export const SYMBOL_DOT:Token = Symbol(&amp;quot;.&amp;quot;);

export type TokenList = Array&amp;lt;{token:Token, value:string}&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// lexer.ts
/// LexDefinitionsの定義は先述のものと同一
import {Token, SYMBOL_EOF, TokenList} from &amp;quot;./token&amp;quot;;
import {LexDefinitions} from &amp;quot;./grammar&amp;quot;;

export interface ILexer{
	exec(str: string):TokenList;
}

export class Lexer implements ILexer{
	constructor(public def: LexDefinitions){
		// 正しいトークン定義が与えられているかチェック
		for(var i=0; i&amp;lt;this.def.length; i++){
			var token_pattern = this.def[i].pattern;
			if(typeof token_pattern == &amp;quot;string&amp;quot;){
				continue;
			}
			else if(token_pattern instanceof RegExp){
				// フラグを整形する
				let flags:string = &amp;quot;&amp;quot;;
				// gフラグは邪魔なので取り除く
				// i,m,uフラグがあれば維持する
				if(token_pattern.ignoreCase){
					flags += &amp;quot;i&amp;quot;;
				}
				if(token_pattern.multiline){
					flags += &amp;quot;m&amp;quot;;
				}
				if(token_pattern.unicode){
					flags += &amp;quot;u&amp;quot;;
				}
				// yフラグは必ずつける
				flags += &amp;quot;y&amp;quot;;
				// フラグをつけなおして新しい正規表現オブジェクトにする
				this.def[i].pattern = new RegExp(token_pattern, flags);
				continue;
			}
			throw new Error(&amp;quot;invalid token definition: neither string nor RegExp object&amp;quot;);
		}
	}
	exec(str: string):TokenList{
		var result:TokenList = [];
		let lastindex = 0;
		while(lastindex &amp;lt; str.length){
			for(var i=0; i&amp;lt;this.def.length; i++){
				var token:Token|null = this.def[i].token;
				var token_pattern = this.def[i].pattern;
				var match:string;
				if(typeof token_pattern == &amp;quot;string&amp;quot;){
					let last_tmp = lastindex+token_pattern.length;
					if(str.substring(lastindex,last_tmp) != token_pattern) continue;
					if(last_tmp &amp;lt; str.length &amp;amp;&amp;amp; /\w/.test(token_pattern.slice(-1)) &amp;amp;&amp;amp; /\w/.test(str[last_tmp])) continue; // ヒットした文字の末尾が\wで、そのすぐ後ろが\wの場合はスキップ
					match = token_pattern;
					lastindex += token_pattern.length;
				}
				else{
					// token_pattern: RegExp
					token_pattern.lastIndex = lastindex;
					let m = token_pattern.exec(str);
					if(m === null) continue; // マッチ失敗
					match = m[0];
					lastindex = token_pattern.lastIndex; // lastindexを進める
				}
				// tokenがnullなら処理を飛ばします
				if(token != null){
					result.push({token:token, value:match});
				}
				break;
			}
		}
		// 最後にEOFトークンを付与
		result.push({token:SYMBOL_EOF, value:&amp;quot;&amp;quot;});
		return result;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まずコンストラクタでは、与えられた字句規則に簡単な型チェックと正規表現の整形を行っています。
グローバルマッチは今回は邪魔なだけなので、与えられた正規表現にgフラグがついていれば取り除きます。
ES2015でRegExpに追加されたstickyフラグ(&lt;strong&gt;ほぼ&lt;/strong&gt;全ての主要モダンブラウザ上で実装済み)を使うと楽なので、ここでyフラグの追加も行います。&lt;/p&gt;

&lt;p&gt;execメソッドでは入力を読み終えるまでマッチングを繰り返し、&lt;code&gt;{token:Token, value:string}&lt;/code&gt; というオブジェクトを結果の配列に追加していきます。
先述のようにパターンが文字列であれば&lt;code&gt;\w&lt;/code&gt;が連続した場所では区切らないようにして、マッチングが成功するたびにインデックス位置を先に進めていきます。&lt;/p&gt;

&lt;p&gt;また、すべての入力を読み終えた後、最後にSymbol(EOF)を名前としたトークンを結果に追加します。
これは入力の末尾を意味するトークンで、構文解析の際に内部的に使用されます。&lt;/p&gt;

&lt;p&gt;(Symbolは、それ自身と比較しない限り&lt;code&gt;==&lt;/code&gt;や&lt;code&gt;===&lt;/code&gt;の評価結果が常にfalseになるプリミティブ型で、ES2015で追加されたものです。
字句規則で定義されたトークンとの衝突が発生しないようにここでSymbolを使用していますが、Symbolそのものはオブジェクトのプロパティとして使用することで後方互換性を維持することを目的としてJavaScriptに追加された型であるため、この用途で用いるのに適しているのかどうかは議論の余地があります。
とはいえプログラミング言語個別の問題はこの記事の主題とは関係がないため、詳しくは言及しません。)&lt;/p&gt;

&lt;p&gt;この字句解析器に先ほどの字句規則を与え、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; を入力すると、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;[
	{token:&amp;quot;NUMBER&amp;quot;, value:&amp;quot;9&amp;quot;},
	{token:&amp;quot;PLUS&amp;quot;, value:&amp;quot;+&amp;quot;},
	{token:&amp;quot;NUMBER&amp;quot;, value:&amp;quot;11&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, value:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, value:&amp;quot;(&amp;quot;},
	{token:&amp;quot;NUMBER&amp;quot;, value:&amp;quot;2&amp;quot;},
	{token:&amp;quot;PLUS&amp;quot;, value:&amp;quot;+&amp;quot;},
	{token:&amp;quot;NUMBER&amp;quot;, value:&amp;quot;1&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, value:&amp;quot;)&amp;quot;},
	{token:Symbol(EOF), value:&amp;quot;&amp;quot;}
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;という結果が得られます。
これでトークンの並びを得ることに成功したので、次回以降はいよいよパーサジェネレータの作成に移っていくことになります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回:かんたん構文解析入門&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第1回:かんたん構文解析入門</title>
      <link>http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/</link>
      <pubDate>Thu, 22 Dec 2016 03:03:09 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/</guid>
      <description>
        

&lt;p&gt;この記事は&lt;a href=&#34;http://www.adventar.org/calendars/1881&#34;&gt;Kobe University Advent Calendar 2016&lt;/a&gt;の21日の記事です。また遅刻か。
なお私は当該大学の学部2年(2016年12月現在)です。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;構文解析ができるプログラマはちょっとかっこいいですよね。
「構文解析？ああ、できますよ」とか言って自分のスキルを自慢できそうな印象があります。&lt;/p&gt;

&lt;p&gt;(ほぼ)フルスクラッチでTypeScriptによるLR(1)パーサジェネレータを実装した(ついでにLALR(1)パーサも作れる)ので、これを完成させるまでの流れを紹介していこうと思います。&lt;/p&gt;

&lt;p&gt;今回は構文解析自体の入門編となります。&lt;/p&gt;

&lt;p&gt;自作したパーサジェネレータは &lt;a href=&#34;https://github.com/Tatamo/parsergenerator&#34;&gt;https://github.com/Tatamo/parsergenerator&lt;/a&gt; にあります。&lt;br /&gt;
今のところパーサジェネレータ部分は完成、基本的な構文解析なら問題なくこなせるので構文規則や字句規則を外部から読み取って構文解析してパーサジェネレータに渡すような処理や、全体の見通しを良くするための設計の見直しやリファクタリング等を行っている段階です。
ドキュメント作ってなくてすみません。&lt;/p&gt;

&lt;h2 id=&#34;構文解析をしたい:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析をしたい&lt;/h2&gt;

&lt;p&gt;構文解析、時々見かけるフレーズです。
プログラマなら覚えておいて損はない技術……かどうかはわかりませんが、そういう類のスキルに(傍からは)見えます。&lt;br /&gt;
ぜひやりましょう。&lt;/p&gt;

&lt;p&gt;ひとまず、何をやりたいのかを明確にする必要があります。&lt;br /&gt;
この記事では、「入力として与えられるLR(1)文法に属する文法に従ったトークン列をパース(構文解析)することで、その構造を構文木として出力する」ことを目標とします。
何を言っているのかさっぱりわかりませんね、わからなくていいです。&lt;/p&gt;

&lt;p&gt;順を追って説明する必要がありますが、詳細は適宜省略します。
そのため、まずは今回主に参照した資料を列挙しておきます。&lt;/p&gt;

&lt;h2 id=&#34;参考資料一覧:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;参考資料一覧&lt;/h2&gt;

&lt;p&gt;より詳しく知りたい方は、下記に挙げる資料やそこで紹介されている参考文献などを参照されるのが良いと思われます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/courses/cs412/2003sp/lectures/lec09.pdf&#34;&gt;Cornell CIS Introduction to Compilers Lecture 9: LR(1) Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jaist.ac.jp/~kshirai/lec/i223/04a.pdf&#34;&gt;JAIST 自然言語処理論Ｉ 4.文法2(構文解析) その1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jaist.ac.jp/~kshirai/lec/i223/04b.pdf&#34;&gt;JAIST 自然言語処理論Ｉ 4.文法2(構文解析) その2&lt;/a&gt;
(注：「LR法による構文解析」として紹介されているアルゴリズムはSLR法)&lt;br /&gt;
上記3つはネット上にアップロードされている特定の大学の講義資料ですが、公開の規定等を確認していないためリンクを張ることに不都合があるようなら知らせていただけると助かります。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_LR_parser&#34;&gt;Canonical LR parser - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ichikaz3/lr-parsing&#34;&gt;LR parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/uhyo_&#34;&gt;うひょ(@uhyo_)さん&lt;/a&gt; 生き字引。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;構文解析とは-ざっくり:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析とは？(ざっくり)&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; という数式を考えてみます。
構文解析をすることによる最終的な目的は、この数式を(たとえば)文字列として与えると、結果としてこの数式の答えが&lt;code&gt;42&lt;/code&gt;であることを導く、といったことです。&lt;/p&gt;

&lt;p&gt;そのためには、以下のものが必要になります：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数式を表現する構文規則&lt;/li&gt;
&lt;li&gt;上記構文規則を解析するように作られた構文解析器(Parser)&lt;/li&gt;
&lt;li&gt;解析された構文を処理するプログラム&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;さらに、これらの構文解析に入る前の下準備のために以下が必要です；&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文字列をトークンとして分割して表現するための字句規則&lt;/li&gt;
&lt;li&gt;上記字句規則をもとに、文字列を読み取ってトークンを返す字句解析器(Lexical Analyzer、略してLexer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ちなみに、今回の記事の目標は、それらに加えて以下のものを実装することです：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;構文規則および字句規則を入力として与えることで、構文解析器そのものを自動生成するパーサジェネレータ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;実際の構文解析を行う手順とはずれてしまいますが、紹介した順番に沿って構文解析器→字句解析器の順に解説していきます。&lt;/p&gt;

&lt;h3 id=&#34;構文解析器-パーサ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析器(パーサ)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; という数式を解析するためには、まずこの数式がどのようなルールで記述されているのかを(再)定義する必要があります。
そのルールをを表すのが構文規則です。
構文規則を書き表すルールは、たとえば&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AB%E3%82%B9%E3%83%BB%E3%83%8A%E3%82%A6%E3%82%A2%E8%A8%98%E6%B3%95&#34;&gt;BNF&lt;/a&gt;など様々な種類がありますが、基本的な発想としては&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; X Y Z
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように左辺の記号を右辺の記号の並びによって定義することで行います。&lt;/p&gt;

&lt;p&gt;具体的に見てみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;式 -&amp;gt; 式 &amp;quot;+&amp;quot; 項
式 -&amp;gt; 項
項 -&amp;gt; 項 &amp;quot;*&amp;quot; 因子
項 -&amp;gt; 因子
因子 -&amp;gt; 数
因子 -&amp;gt; &amp;quot;(&amp;quot; 式 &amp;quot;)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;妥当ですね。
&lt;code&gt;式 -&amp;gt; 式 + 項&lt;/code&gt; と &lt;code&gt;式 -&amp;gt; 項&lt;/code&gt;の２つの規則が、再帰的な繰り返しを表現していることに注意してください。
たとえば、&lt;code&gt;項&lt;/code&gt;は当然&lt;code&gt;式&lt;/code&gt;ですし、&lt;code&gt;項 + 項&lt;/code&gt;も&lt;code&gt;式(-&amp;gt;項) + 項&lt;/code&gt; より&lt;code&gt;式&lt;/code&gt;となります。
さらに、&lt;code&gt;項 + 項 + 項&lt;/code&gt;は最初の&lt;code&gt;項 + 項&lt;/code&gt;が&lt;code&gt;式&lt;/code&gt;なので、&lt;code&gt;式(-&amp;gt;項 + 項) + 項&lt;/code&gt; より&lt;code&gt;式&lt;/code&gt;です。
よって、&lt;code&gt;式&lt;/code&gt;は&lt;code&gt;項&lt;/code&gt;を&lt;code&gt;&amp;quot;+&amp;quot;&lt;/code&gt;によって任意の回数だけ繋げたものであり、同様に&lt;code&gt;項&lt;/code&gt;は&lt;code&gt;因子&lt;/code&gt;を&lt;code&gt;&amp;quot;*&amp;quot;&lt;/code&gt;で繋げたものとなります。
最後に、&lt;code&gt;因子&lt;/code&gt;は単なる&lt;code&gt;数&lt;/code&gt;かもしれませんし、または&lt;code&gt;&amp;quot;(&amp;quot;&lt;/code&gt;と&lt;code&gt;&amp;quot;)&amp;quot;&lt;/code&gt;で囲まれた&lt;code&gt;式&lt;/code&gt;かもしれません。
これは括弧で囲まれた部分の式が他の部分よりも高い優先順位となることを表現しています。&lt;/p&gt;

&lt;p&gt;たとえば&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt;は、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;式{ [9] [+] [11 * (2 + 1)] }
式{ 項{ [9] } &amp;quot;+&amp;quot; 項{ [11] [*] [(2 + 1)] } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ [(] [2 + 1] [)] } } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ [2] [+] [1] } &amp;quot;)&amp;quot; } } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ 項{ [2] } &amp;quot;+&amp;quot; 項{ [1] } } &amp;quot;) &amp;quot;} } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ 項{ 因子{2} } &amp;quot;+&amp;quot; 項{ 因子{1} } } &amp;quot;)&amp;quot; } } }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように展開されます(こうして得られた構造をどう解析するかについては省略します)。この、解析対象→構文木の変換を自動で行うのがパーサです。&lt;/p&gt;

&lt;p&gt;ちなみにですが、この構文規則は解析したい対象ごとにあなたが一から書き上げる必要があります。&lt;/p&gt;

&lt;h3 id=&#34;字句解析器-レキシカルアナライザ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;字句解析器(レキシカルアナライザ)&lt;/h3&gt;

&lt;p&gt;上記構文規則では、&lt;code&gt;+&lt;/code&gt;や&lt;code&gt;*&lt;/code&gt;のような演算子、&lt;code&gt;数&lt;/code&gt;についての規定はありません。
これらの「左辺に現れない記号」を、「終端記号」と呼びます。左辺に現れる記号は非終端記号と呼ばれます。&lt;/p&gt;

&lt;p&gt;通常、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; のような入力は文字列で与えられますが、記号と記号の間には複数もしくは0個の空白が挿入されている可能性もあります。
しかし以下のような構文規則を定義するのは本質的ではありません：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;空白 -&amp;gt; &amp;quot; &amp;quot;
空白 -&amp;gt; &amp;quot; &amp;quot; 空白
数字 -&amp;gt; &amp;quot;0&amp;quot; | &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | ... | &amp;quot;9&amp;quot;
数字 -&amp;gt; (&amp;quot;0&amp;quot; | &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | ... | &amp;quot;9&amp;quot;) 数字
数 -&amp;gt; (&amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | &amp;quot;3&amp;quot; ... | &amp;quot;9&amp;quot;) 数字
ただし、|は「または」を表す
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;そこで、通常は「入力として与えられた文字列」を「終端記号として分類されたトークンの列」に変換する処理をはさみ、これによって得られたトークンを構文解析器に与えます。
トークンとは終端記号と、必要ならばそれに紐付いた元々の情報を保持しておいたものです。たとえば、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt;は、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: 9
プラス: +
数字: 11
アステリスク: *
左括弧: (
数字: 2
プラス: +
数字: 1
右括弧: )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というような9つのトークンの列に分けることができます。
構文解析器はそのトークンがどのような終端記号に対応しているかは見ますが、たとえば個々の数字が何であるかを判断することはしません。
これによって、構文解析器は本質的な文法の解析のみに注力することができます。&lt;/p&gt;

&lt;p&gt;この処理をするのが字句解析器で、どのような文字や文字列が与えられた場合に何という終端記号かを判別するための規則が字句規則です。&lt;/p&gt;

&lt;p&gt;字句規則は、例えば以下のような書き方になるでしょう：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: /[1-9][0-9]*/
プラス: &amp;quot;+&amp;quot;
アステリスク: &amp;quot;*&amp;quot;
左括弧: &amp;quot;(&amp;quot;
右括弧: &amp;quot;)&amp;quot;
(読み捨て): /\s/
(不正): /./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは字句規則の表現のために、文字列および正規表現を使用しています。
通常(?)字句規則は上から順に文字列の先頭部分を当てはめていき、マッチするものがあればその終端記号に対応付けます。
そのため、&lt;code&gt;(不正)&lt;/code&gt;の部分は入力された文字全てにマッチする正規表現&lt;code&gt;/./&lt;/code&gt;が使用されていますが、これは上の規則のいずれにも当てはまらなかった場合にのみマッチします。&lt;/p&gt;

&lt;p&gt;与えられた文字列を前から順番に見ていくだけなので、字句解析器の実装はパーサやパーサジェネレータの実装と比べると単純です。&lt;/p&gt;

&lt;h3 id=&#34;パーサジェネレータ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;パーサジェネレータ&lt;/h3&gt;

&lt;p&gt;ここまで構文解析器(パーサ)と字句解析器(レキシカルアナライザ)について見てきました。
基本的にはこの2つによって構文解析を行うことができ、基本的な流れとしては&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;入力となるような解析したい言語を用意する&lt;/li&gt;
&lt;li&gt;字句規則を用意して、それをもとにしたレキシカルアナライザを用意する&lt;/li&gt;
&lt;li&gt;レキシカルアナライザに入力を与え、トークンの列を取得する&lt;/li&gt;
&lt;li&gt;構文規則を用意して、それをもとにしたパーサを用意する&lt;/li&gt;
&lt;li&gt;パーサにトークンの列を与え、解析結果を得る&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;となります。
パーサジェネレータとは何かというと、この 4. の部分を自動化するものです。
(LR法の)構文解析器は、内部的には入力を受け取ってスタックに積みながら状態遷移を繰り返すオートマトンにすぎません。
そのため、どの入力が与えられればどのような状態に遷移するかを示す「構文解析表」を得ることができれば、その構文を解析するパーサを作成することができます。
パーサジェネレータは、構文規則を読み取ることでこの構文解析表をつくり上げるという処理を主に行います。&lt;/p&gt;

&lt;p&gt;字句解析器程度ならわざわざジェネレータを作らなくても、字句規則そのものを字句解析器に渡せば良い感じに字句解析してくれるようにできますが、パーサジェネレータも「構文解析表の構築後、それをもとにして構文解析を行う」ような機能がついていればそれはパーサであるとも言えます。
わざわざパーサとパーサジェネレータが分けられているのは、一つには計算資源の乏しかった昔はパーサジェネレータがオンメモリで展開した構文解析表をもとにそのままパーサとして振る舞うというようなことが少なく、構文解析表を与えることで「パーサのソースコード」を出力するようなものが一般的だったからではないかと思われます(適当な思いつきを言っています)。
もっとも、現在でもパーサジェネレータがパーサを生成する際の処理には時間がかかるため、予めパーサをコンパイルしておけるようにするのは妥当といえるでしょう。
字句解析器のための「字句解析器ジェネレータ」も実際に存在していますが、ここでは簡単のために字句解析器はコンストラクタに字句規則を与えれば勝手に良い感じの字句解析を行ってくれるようになるものと思ってもらえればよいです。&lt;/p&gt;

&lt;h3 id=&#34;文脈自由言語について:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;文脈自由言語について&lt;/h3&gt;

&lt;p&gt;構文解析器が解析対象とする「言語」がどのようなものであるかについてはいろいろな定義がなされています。&lt;/p&gt;

&lt;p&gt;これについては、参考資料でも紹介した&lt;a href=&#34;https://twitter.com/ki6o4&#34;&gt;うさぎさん(@ki6o4)&lt;/a&gt;の&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が詳しいため、こちらを参照していただくことをおすすめします。
ここでは、厳密な話はあまりせずにごくごく簡単に触れていこうと思います。&lt;/p&gt;

&lt;p&gt;構文解析の対象とするのは、基本的に文脈自由言語となります。
構文解析の手法にも様々なものがありますが、それらの手法の中には文脈自由言語すべてを解析できるわけではないものも多く、たとえばLR(1)法ならLR(1)文法やLR(1)言語というように、ある手法で解析できる文法や、解析できる言語全体をその手法の名前で表される言語として表現することがあります。&lt;/p&gt;

&lt;h3 id=&#34;解析手法と言語のクラス:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;解析手法と言語のクラス&lt;/h3&gt;

&lt;p&gt;いくつかの手法を主観を交えて乱暴に紹介していきます。&lt;/p&gt;

&lt;h4 id=&#34;先読み:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;先読み&lt;/h4&gt;

&lt;p&gt;**この項は下のLR法などの項を「先読み」してから戻ってきて読むことをおすすめします**&lt;/p&gt;

&lt;p&gt;たとえばLR(1)法のように、数字を括弧でくくって(k)と表現している手法がいくつかあります。このkは何文字先読みするかを示していて、たとえば(1)ならば1文字先読みするという意味です。
先読み数については、たとえばLR法については以下のようなことが言われています。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;LR(k)で表せる文法のクラス ⊆ LR(k+1)先読みで表せるクラス である&lt;/li&gt;
&lt;li&gt;LR(k)文法によって受理可能な言語のクラスは、LR(1)のそれと等しい&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2.より、基本的には(1)について考えることが多いようです。&lt;/p&gt;

&lt;h4 id=&#34;ll-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LL(1)法&lt;/h4&gt;

&lt;p&gt;「再帰を使って構文解析する」という発想としては単純なもの。
LL(1)文法のクラスはLR(1)よりも大幅に小さいものの、それでもLALR(1)文法を外れた文法を解析できたりします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; S + E
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というような、右辺の一番左の場所に左辺の記号が登場するような「左再帰則」を読むことができません。ナンセンス。&lt;/p&gt;

&lt;h4 id=&#34;lr-0-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(0)法&lt;/h4&gt;

&lt;p&gt;先読み数が0なのでよわい。&lt;/p&gt;

&lt;h4 id=&#34;slr法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;SLR法&lt;/h4&gt;

&lt;p&gt;SLRのSはSimpleの意味です。LR(0)から単純な先読みを加えることでLR(0)よりも解析可能な文法が増えますが、それでもLALR(1)には及びません。&lt;/p&gt;

&lt;h4 id=&#34;lr-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(1)法&lt;/h4&gt;

&lt;p&gt;LR(0)に対し、1文字だけ先読みして次にどのような入力が期待されるかを判断。
LR(1)文法がそれなりに広いという点で優秀な一方、LR(0)に比べて構文解析表の大きさが爆発しやすいという欠点がある、と言われています。
しかし今の時代はそんなものは大した欠点になり得ない気がします。&lt;/p&gt;

&lt;h4 id=&#34;lalr-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LALR(1)法&lt;/h4&gt;

&lt;p&gt;プログラミング言語を解析するコンパイラなどによく使われている手法です。
LALR(1)のLAはLook-Aheadの略で、まずLR(1)法で構文解析表を作ってから、文法部分が同じで先読み記号だけが違うような状態をマージするという点がLR(1)法と異なります。
表を併合してしまうためにLR(1)法よりも解析可能な文法のクラスが小さくなるものの、実用上はほとんど問題にならず、LR(1)法の構文解析表が大きくなりすぎるという欠点を補える手法です。&lt;/p&gt;

&lt;p&gt;ただし、一度LR(1)法の表を作ること変わりはないのでメモリ消費量はそう変わらないし、大きなデータも問題なく扱える今の時代にわざわざ構文解析表を数十パーセント程度削減したところで何の意味があるのかという疑問があります。&lt;/p&gt;

&lt;p&gt;また、LALR法のLAはLook-Aheadの略だと言いましたが、注意しなければならないのは&lt;strong&gt;Look-Ahead(先読み)を行うのはLALR法固有の手法ではない&lt;/strong&gt;ということです。  先読み自体はLR(1)法でもやりますし、LALR(1)はあくまでLR(1)の先読み部分をマージしたものにすぎません。
私はLALR法の名前の付け方はあまり良くないと思っていて、MLR法(Merged Look-Ahead LR法)とかなんとか、そういう感じの名前に変えたほうが良いと思います。&lt;/p&gt;

&lt;h4 id=&#34;glr法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;GLR法&lt;/h4&gt;

&lt;p&gt;「あいまいな」解釈が可能な文法があった場合、考えられうるすべての可能性を探索してしまうことによって解決する手法。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;「お魚くわえた猫を追いかけるサザエさん」&lt;/code&gt;で魚をくわえているのが猫とサザエさんの両方に解釈できるように、一つの入力に対して複数の結果が得られることがあります。
どちらかというと自然言語処理向きかもしれません。&lt;/p&gt;

&lt;h4 id=&#34;cyk法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;CYK法&lt;/h4&gt;

&lt;p&gt;強力なアルゴリズムにより、文脈自由言語すべてを比較的高速に解析可能。
ただし、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; NP VP
VP -&amp;gt; v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように、チョムスキー標準形といわれるような、「右辺が非終端記号ちょうど2つか終端記号1つでなければならない」、つまり結果として得られる構造が二分木になっていなければならないというナンセンスにも程がある制約を課されます(アルゴリズムの改良や規則の変換、得られた木構造の後処理などによって回避は可能ですが)。&lt;/p&gt;

&lt;p&gt;このCYK法の計算量オーダーは&lt;code&gt;O(n^3)&lt;/code&gt;程度で、文脈自由言語全てを解析可能なアルゴリズムの中では高速ですが、この記事で紹介されている他のアルゴリズムよりは低速となります。
たとえばLR(1)法は文脈自由言語全体を解析出来ないかわりに&lt;code&gt;O(n)&lt;/code&gt;で解析が可能です。&lt;/p&gt;

&lt;p&gt;プログラミング言語の解析では、言語の開発者が文法自体をある程度自由に定義することができるため、文脈自由言語の一部だけでなく全体を解析したいという需要はあまり発生しません。&lt;/p&gt;

&lt;h2 id=&#34;lr-1-パーサジェネレータをつくろう:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(1)パーサジェネレータをつくろう&lt;/h2&gt;

&lt;p&gt;構文解析の大まかな流れはわかりました。
とりあえず字句規則と構文規則を用意して、あとはどうにかしてこの構文を読んでくれるようなパーサを用意すれば構文解析ができそうです(字句解析器なんてのは適当にやってもすぐ用意できます)。&lt;/p&gt;

&lt;p&gt;先ほど紹介したような手法によって構文ごとに一からパーサをプログラミングするようなことはやりたくないので、パーサジェネレータを用いてパーサを自動的に生成してもらえば事は済みそうですね。&lt;/p&gt;

&lt;p&gt;ここに&lt;a href=&#34;https://www.gnu.org/software/bison/&#34;&gt;Bison&lt;/a&gt;という有名なパーサジェネレータがあります。
今の時代にわざわざCやC++で構文解析なんてしたくないのでしたら、Pythonで&lt;a href=&#34;http://www.dabeaz.com/ply/&#34;&gt;PLY&lt;/a&gt;とか、JavaScriptの&lt;a href=&#34;https://github.com/zaach/jison&#34;&gt;jison&lt;/a&gt;というものなど、いくらでも選択肢があります。
これらのうち一つを選んで、チュートリアルを読んでパーサを作っていくのがいいでしょう。&lt;/p&gt;

&lt;p&gt;というわけで、前段を書いていると結構分量が膨らんでしまったため、今回はここで区切ります。&lt;/p&gt;

&lt;p&gt;では次回からは、構文解析を行えるようになるため、LR(1)法を用いたパーサジェネレータを実際に作っていく流れを紹介していきたいと思います。&lt;/p&gt;

&lt;p&gt;えっちょっとまって、今パーサジェネレータは既存のものを使えばいいって言ったよね、ねえ&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;次回:字句解析器の実装&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>