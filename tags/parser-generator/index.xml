<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Parser Generator on わたしろぐ</title>
    <link>http://tatamo.81.la/blog/tags/parser-generator/</link>
    <description>Recent content in Parser Generator on わたしろぐ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Wed, 22 Mar 2017 15:58:04 +0900</lastBuildDate>
    <atom:link href="http://tatamo.81.la/blog/tags/parser-generator/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第4回:かんたんLR(1)法入門</title>
      <link>http://tatamo.81.la/blog/2017/03/22/lr-parser-generator-implementation-04/</link>
      <pubDate>Wed, 22 Mar 2017 15:58:04 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2017/03/22/lr-parser-generator-implementation-04/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/&#34;&gt;前回&lt;/a&gt;で構文解析器を生成する際に必要となる準備を済ませたため、LR(1)法ベースのパーサジェネレータを作る用意が整いました。
ですが相変わらず本題のパーサジェネレータ作成には入らず、まずはLR(1)法のおおまかな理論的概略の紹介を行います。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;では構文解析全体の流れを解説しましたが、実際にどのような過程でパーサを、またパーサジェネレータを作成するかについては触れませんでした。
今回は、LR法による構文解析の流れを解説するとともに、これからどのような流れでパーサジェネレータを作成していくのかを紹介します。
今回は解説のみのためソースコードが載りません。&lt;/p&gt;

&lt;h2 id=&#34;lr-1-構文解析の流れ:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;LR(1)構文解析の流れ&lt;/h2&gt;

&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34; data-lang=&#34;ja&#34;&gt;&lt;p lang=&#34;ja&#34; dir=&#34;ltr&#34;&gt;字句規則を用意して字句解析器にかけてトークン列を取得したあと構文規則をもとにFIRST関数とFOLLOW関数を求め、それをもとにgotoグラフを導出することによってLR表を作成して、構築したLRパーサでトークン列を解析して得た抽象構文木を処理すれば構文解析ができると知ったわたし &lt;a href=&#34;https://t.co/aIbxqSf5qj&#34;&gt;pic.twitter.com/aIbxqSf5qj&lt;/a&gt;&lt;/p&gt;&amp;mdash; たたも (@&lt;strong&gt;tatamo&lt;/strong&gt;) &lt;a href=&#34;https://twitter.com/__tatamo__/status/798837425313189888&#34;&gt;2016年11月16日&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;FOLLOW関数はSLR法などで使用する概念のため、LR(1)法を用いる今回の記事では用いません。忘れてください。&lt;/p&gt;

&lt;p&gt;LR構文解析の流れは、以下の通りとなります。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First関数を求める&lt;/li&gt;
&lt;li&gt;アイテム集合およびDFA(gotoグラフ)を作成する&lt;/li&gt;
&lt;li&gt;(LALR法のみ)DFAの先読み部分をマージし、より状態数が少なく軽量なDFAにする&lt;/li&gt;
&lt;li&gt;DFAをもとに構文解析表(LR表)を構築する&lt;/li&gt;
&lt;li&gt;構文解析表を実行できるパーサを作成する&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1.のFirst関数については、&lt;a href=&#34;http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/&#34;&gt;前回&lt;/a&gt;の記事で紹介を済ませているため割愛します。&lt;br /&gt;
LALR法のLR(1)法との相違点は3.のみで、他はLR(1)法と全く同じ処理を行います。&lt;/p&gt;

&lt;p&gt;パーサジェネレータを作成して解析する構文を自由に決定できるようにする場合、3.の構文解析表までを与えられた構文に合わせて自動的に生成できるようにします。&lt;/p&gt;

&lt;h2 id=&#34;アイテム集合とdfa:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;アイテム集合とDFA&lt;/h2&gt;

&lt;p&gt;LR法による構文解析のためには、DFA(決定性有限オートマトン)の作成を行う必要があります。&lt;/p&gt;

&lt;p&gt;LR法によって作られるDFAは、それぞれの状態(ノード)に、アイテム集合と呼ばれる情報と、他の状態への遷移ルールを示すトークンをラベルとした辺情報とを持ちます。
このLRアイテム集合は、構文解析表やDFA自身の構築のために必要な情報として使用されます。&lt;/p&gt;

&lt;p&gt;アイテム集合は、文字通りアイテム(便宜的にLRアイテムと呼称します)からなる集合です。&lt;/p&gt;

&lt;h3 id=&#34;lrアイテム:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;LRアイテム&lt;/h3&gt;

&lt;p&gt;個別のLRアイテムは、以下のようなものです。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;X -&amp;gt; A . B C [x,y,$]
Xは非終端記号
A,B,Cは終端記号または非終端記号
x,yは終端記号
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一見すると&lt;code&gt;X -&amp;gt; A B C&lt;/code&gt;のような構文規則のルールのように見えますが、相違点があります。&lt;/p&gt;

&lt;p&gt;まず、規則の右辺に&lt;code&gt;.&lt;/code&gt;という記号が存在します。
これは終端記号でも非終端記号でもなく、「現在この部分まで解析した」ということを示すマーカーです。
上記の場合、&lt;code&gt;X&lt;/code&gt;という記号の解析の途中で既に&lt;code&gt;A&lt;/code&gt;を読み終え、次は&lt;code&gt;B C&lt;/code&gt;が与えられることが期待されているということを意味します。&lt;/p&gt;

&lt;p&gt;次に、規則の右辺のさらに右に、&lt;code&gt;[x,y,$]&lt;/code&gt;という表記が存在します。
これはLR(1)法の(1)先読みのために用いる先読み記号の集合を表しています。
解析が進んで&lt;code&gt;X&lt;/code&gt;の解析が終わった場合、つまり&lt;code&gt;.&lt;/code&gt;の位置が右端まで移動した場合、その次には&lt;code&gt;x&lt;/code&gt;,&lt;code&gt;y&lt;/code&gt;,&lt;code&gt;$&lt;/code&gt;のいずれかの記号が来ることを意味します。
先読み記号は常に終端記号であることに注意してください。
また、&lt;code&gt;$&lt;/code&gt;は「入力の終わり」を表す記号で、これは便宜的に終端記号として扱います(&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;第2回&lt;/a&gt;で内部的に追加したSymbol(EOF)トークンのことです)。&lt;/p&gt;

&lt;h3 id=&#34;dfaの構築:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;DFAの構築&lt;/h3&gt;

&lt;p&gt;まず、DFAを最初の状態で初期化します。
このとき、DFAのノード数は一つのみであり、そのノードは、以下のようなLRアイテム一つのみを要素とするアイテム集合を持ちます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S&#39; -&amp;gt; . S [$]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ただし、&lt;code&gt;S&lt;/code&gt;は開始記号であり、&lt;code&gt;S&#39;&lt;/code&gt;は便宜的に追加した新しい非終端記号です。
便宜的には、&lt;code&gt;S&#39;&lt;/code&gt;について以下の規則が成り立つこととみなします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S&#39; -&amp;gt; S $
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを自己展開させることによって、構文解析のためのDFAを構築していきます。&lt;/p&gt;

&lt;h4 id=&#34;クロージャー展開:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;クロージャー展開&lt;/h4&gt;

&lt;p&gt;まず、初期化時点で存在するこのDFAノードは、まだ完全な状態にはなっていません。
一定のルールに従い、アイテム集合を「クロージャー展開」する必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;X -&amp;gt; α . Y β [x]
X,Yは非終端記号(X=Yであってもよい)
xは終端記号
α,βは任意の長さの終端記号または非終端記号の列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というLRアイテムが存在する場合、&lt;code&gt;Y&lt;/code&gt;を左辺として&lt;code&gt;.&lt;/code&gt;が右辺の左端にあるような新しいLRアイテムを、アイテムセットに追加します。
ただし、先読み記号はFirst(βx)で得られる記号全てとします。
つまり、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;Y -&amp;gt; γ
γは任意の長さの終端記号または非終端記号の列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というような規則があった場合、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;Y -&amp;gt; . γ [First(βx)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というLRアイテムを新しく追加します。&lt;/p&gt;

&lt;p&gt;これを、新しいアイテムが追加されなくなるまで繰り返します。&lt;/p&gt;

&lt;p&gt;具体的に見て行きましょう。
以下の規則を仮定します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S -&amp;gt; 0
S -&amp;gt; X 1
X -&amp;gt; 0
Sは開始記号
S,Xは非終端記号
0,1は終端記号
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この場合、開始記号は&lt;code&gt;S&lt;/code&gt;なので、最初のLRアイテムは以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S&#39; -&amp;gt; . S [$]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;.&lt;/code&gt;の次にある&lt;code&gt;S&lt;/code&gt;を展開します。
先読み記号は&lt;code&gt;First($)=[$]&lt;/code&gt;です。
以下のアイテムを追加します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S -&amp;gt; . 0 [$]
S -&amp;gt; . X 1 [$]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;さらに、新しく追加されたアイテムにも同様の処理を行うと、&lt;code&gt;.&lt;/code&gt;の次に&lt;code&gt;X&lt;/code&gt;があるため、これを展開します。
&lt;code&gt;0&lt;/code&gt;は終端記号のため、展開は行いません。
先読み記号は、&lt;code&gt;First(1$)=[1]&lt;/code&gt;です(First関数は終端記号の列の左端の記号を得るので、ここでは&lt;code&gt;1&lt;/code&gt;のみとなります)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;X -&amp;gt; . 0 [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上の規則では&lt;code&gt;0&lt;/code&gt;は終端記号のため、ここで展開は終了します。&lt;/p&gt;

&lt;p&gt;結果として、最初のDFAノードの持つアイテム集合は以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S&#39; -&amp;gt; . S [$]
S -&amp;gt; . 0 [$]
S -&amp;gt; . X 1 [$]
X -&amp;gt; . 0 [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上がクロージャー展開の処理です。
こうして展開したアイテム集合をもとに、新しいDFAノードを生成していきます。&lt;/p&gt;

&lt;h4 id=&#34;新しいdfaノードの生成:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;新しいDFAノードの生成&lt;/h4&gt;

&lt;p&gt;クロージャー展開が完了したアイテム集合から、一定のルールのもとで新しいDFAノードを生成します。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;X -&amp;gt; α . A β [x]
Xは非終端記号
Aは終端記号または非終端記号(X=Aであってもよい)
xは終端記号
α,βは任意の長さの終端記号または非終端記号の列
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というLRアイテムが存在する場合、以下の新しいLRアイテムを生成します(そのDFAノードのアイテムセットには追加しません)。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;X -&amp;gt; α A . β [x]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;そのDFAノードの持つ全てのLRアイテムについてこの処理が終わったら、&lt;code&gt;.&lt;/code&gt;の左隣の記号、つまり&lt;code&gt;A&lt;/code&gt;の位置の記号ごとに新しいアイテム集合を作り、それを情報としてもつ新しいDFAノードを生成します。
そして既存のDFAノードから、&lt;code&gt;A&lt;/code&gt;をラベルとして新しいノードに対して辺を張ります。&lt;/p&gt;

&lt;p&gt;具体的には、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S&#39; -&amp;gt; . S [$]
S -&amp;gt; . 0 [$]
S -&amp;gt; . X 1 [$]
X -&amp;gt; . 0 [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というアイテム集合を持つDFAからは、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;S&#39; -&amp;gt; S . [$]
S -&amp;gt; 0 . [$]
S -&amp;gt; X . 1 [$]
X -&amp;gt; 0 . [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;という4つのLRアイテムが生成され、これは以下の3つに分けられます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;1. ラベル: S
S&#39; -&amp;gt; S . [$]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;2. ラベル: 0
S -&amp;gt; 0 . [$]
X -&amp;gt; 0 . [1]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;3. ラベル: X
S -&amp;gt; X . 1 [$]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このようにして新しく3つのDFAノードを生成し、もとのノードからそれぞれの記号をラベルとした辺を張ります。
あとは、新しいノード全てについて、同様にクロージャー展開を行い、さらに新しいDFAノードを生成していきます。
ただし、その過程で既存のノードと全く同じアイテム集合を持つDFAノードが作られた場合は、新しいノードとしてそこに辺を張るのではなく、かわりに重複する既存のノードに対して辺を張るものとします。&lt;/p&gt;

&lt;p&gt;この処理を繰り返し、DFAノードが新しく生成されなくなればDFAの構築は終了です。&lt;/p&gt;

&lt;h4 id=&#34;lalr法のみ-先読み部分のマージ:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;(LALR法のみ)先読み部分のマージ&lt;/h4&gt;

&lt;p&gt;LALR法では、この時点でDFAのサイズ縮小を行います。
そのアルゴリズムは、以下の通りです。&lt;/p&gt;

&lt;p&gt;まず、DFAの持つアイテム集合から、それぞれのLRアイテムの先読み部分のみを除いた場合に、全く同じアイテム集合を持つようなDFAノードの組を見つけます。
そして、そのようなDFAノードの組において、LRアイテムの先読み部分をそれぞれの和集合とするような新しいDFAノードを作り、それらのノードに対して辺を張っていたノードがあれば、その辺を新しいノードに対して張り直します。&lt;/p&gt;

&lt;p&gt;具体的に、以下のようなアイテム集合を持つ2つのDFAノードを考えます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;DFAノードA:
X -&amp;gt; .Y Z [x]
Y -&amp;gt; .V W [x,y]

DFAノードB:
X -&amp;gt; .Y Z [z]
Y -&amp;gt; .V W [y,z]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この2つのDFAノードは、先読み部分を除けば一致しているため、マージして次のDFAノードCを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;DFAノードC:
X -&amp;gt; .Y Z [x,z]
Y -&amp;gt; .V W [x,y,z]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;そして、DFAノードAまたはBに対して辺を張っているDFAノードが存在するならば、それらの辺をDFAノードCに向けたものに書き換えます。&lt;/p&gt;

&lt;h2 id=&#34;構文解析表と構文解析器:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;構文解析表と構文解析器&lt;/h2&gt;

&lt;p&gt;DFAが完成したら、それをもとにして構文解析表を生成していきます。
構文解析表はそれ自体がオートマトンの動作仕様を表すものであり、構文解析表が完成してしまえば、それに沿ってオートマトンを動作させることで構文解析が可能となります。&lt;/p&gt;

&lt;p&gt;(※&lt;br /&gt;
まあそもそもDFA自体もオートマトンなのですが…&lt;br /&gt;
決定性有限オートマトンとプッシュダウン・オートマトンは別物ですし、役割的にも、構文解析の個々の段階のみを処理できるDFAを多重化したものが(構文解析器という意味での)オートマトンです。&lt;br /&gt;
オートマトンという語が複数の概念を指していて申し訳ありませんが、本記事では「DFA」は構文解析表の前段階として構築されるものを、「オートマトン」は(抽象的・概念的な)構文解析器を指すものとして使用しています。&lt;br /&gt;
たとえ紛らわしくても、物事には名前をつけないといけませんし、それを示す名前がないよりはよっぽどマシなのです。)&lt;/p&gt;

&lt;h3 id=&#34;構文解析を行うオートマトン:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;構文解析を行うオートマトン&lt;/h3&gt;

&lt;p&gt;構文解析表には、オートマトンの現在の状態、および次の入力に応じて、4種類の命令のいずれかが記述されます。
構文解析を行うオートマトンは、現在の状態を示すスタックと、構文解析の結果を保持するスタックの2つのスタックを持ちます。
また、入力を一文字だけ確認するか、入力を消費して一文字先に進めることができます。
(オートマトンの仕様自体は変更の余地があるものと思われます。)&lt;/p&gt;

&lt;p&gt;オートマトンは、構文解析表から(状態スタックの一番上にある状態, 現在見ている入力)の命令を実行します。
最初は(初期状態, 一文字目の入力)となります。&lt;/p&gt;

&lt;p&gt;以下に、4つのそれぞれの命令の説明を記します。
とはいえ、オートマトンの仕様なんざ読んでいて動きが分かるわけもなく楽しくも何ともないため、参考資料の&lt;a href=&#34;http://www.slideshare.net/ichikaz3/lr-parsing&#34;&gt;LR parsing&lt;/a&gt;のスライドを確認していただくことをおすすめします。
オートマトンの動きを視覚的に追いかけることができて非常にわかりやすいです。&lt;/p&gt;

&lt;h4 id=&#34;shift命令:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;shift命令&lt;/h4&gt;

&lt;p&gt;shift命令を受けると、オートマトンは入力を一つ消費します。
shift命令には状態番号が付与されているので、オートマトンは状態スタックにその数値を追加します。&lt;/p&gt;

&lt;h4 id=&#34;reduce命令:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;reduce命令&lt;/h4&gt;

&lt;p&gt;reduce命令は文法idが付与されています。
オートマトンがreduce命令を受けると、示された文法規則を確認し、その右辺の記号の数だけ状態スタックからポップして取り除きます。
さらに、結果スタックからも右辺の記号の数だけ取り除き、取り除いた結果すべてを現在見ている規則の左辺の記号を親とする木構造の子にして、そうしてできた木を結果スタックに追加します(または、取り除いた結果および文法idを引数として何らかのプログラムを実行し、その結果をスタックに追加する場合もあります)。&lt;/p&gt;

&lt;p&gt;そしてその処理の終了後、構文解析表の(状態スタックの一番上にある状態、規則の左辺の記号)の位置にあるgoto命令を実行させます。&lt;/p&gt;

&lt;h4 id=&#34;goto命令:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;goto命令&lt;/h4&gt;

&lt;p&gt;goto命令は、reduce命令の直後に実行されることが期待されます。
goto命令には状態番号が付与されているので、オートマトンは状態スタックにその数値を追加します。
shift命令と異なり、入力の消費は行いません。&lt;/p&gt;

&lt;h4 id=&#34;accept命令:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;accept命令&lt;/h4&gt;

&lt;p&gt;オートマトンがaccept命令を受けると、それは構文解析が終了したことを意味します。
理想的な入力が与えられた場合、入力は全て消費され、結果スタックには最終的な構文解析結果のみが入っていることが期待されます。&lt;/p&gt;

&lt;h3 id=&#34;構文解析器:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;構文解析器&lt;/h3&gt;

&lt;p&gt;構文解析器は、上記の仕様をなぞって構文解析表を読み取ることのできるオートマトンそのものです。
よって、構文解析表さえ個々の構文にあわせて生成することができれば、それを構文解析器に与えることによってさまざまな構文の解析が可能になります。&lt;/p&gt;

&lt;h3 id=&#34;構文解析表の構築:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;構文解析表の構築&lt;/h3&gt;

&lt;p&gt;完成したDFAをもとにして、構文解析表を生成することができます。&lt;/p&gt;

&lt;h4 id=&#34;shiftおよびgotoオペレーションの登録:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;shiftおよびgotoオペレーションの登録&lt;/h4&gt;

&lt;p&gt;それぞれのDFAノードは、オートマトンの状態と対応しています。
簡単のため、個々のDFAノードには一意なid(オートマトンの状態番号)が割り振られているものとします。
すべてのDFAノードについて、そのノードから張られている辺を参照します。&lt;/p&gt;

&lt;p&gt;その辺のラベルの記号が終端記号であるならば、構文解析表の(そのDFAノードのid, ラベルの記号)の部分にshift命令を書き込み、その辺の向かう対象となるDFAノードのidを付与します。&lt;/p&gt;

&lt;p&gt;その辺のラベルの記号が非終端記号であるならば、同様にしてgoto命令を書き込みます。&lt;/p&gt;

&lt;h4 id=&#34;acceptおよびreduceオペレーションの登録:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;acceptおよびreduceオペレーションの登録&lt;/h4&gt;

&lt;p&gt;すべてのDFAノードについて、そのアイテム集合の持つLRアイテム一つ一つを確認していきます。
もしも&lt;code&gt;.&lt;/code&gt;の位置が右辺の末尾にある場合、そのLRアイテムの持つ先読み記号それぞれについて、以下の処理を行います。&lt;/p&gt;

&lt;p&gt;構文解析表の(そのDFAノードのid, 先読み記号)の部分にreduce命令を書き込み、そのLRアイテムのもととなっている規則のidを付与。&lt;/p&gt;

&lt;p&gt;ただし、その規則が&lt;code&gt;S&#39;&lt;/code&gt;に対応するものであった場合、かわりにaccept命令を書き込みます。&lt;/p&gt;

&lt;h4 id=&#34;shift-reduceコンフリクト:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;shift/reduceコンフリクト&lt;/h4&gt;

&lt;p&gt;shiftオペレーションおよびreduceオペレーションは、表の同じ位置に競合して書き込まれてしまうことがあります。
このような状況を、shift/reduceコンフリクトと呼びます。
なお、shift/reduceコンフリクトだけでなく、reduce/reduceコンフリクト、複数回競合しあった3つ以上の命令のコンフリクト等も発生する可能性があります(shift/shiftコンフリクトも発生する可能性があると聞きましたが、上記のアルゴリズムでshiftを登録している場合はDFAが壊れていない限り発生し得ない気がします)。&lt;/p&gt;

&lt;p&gt;コンフリクトが発生してしまった場合の対処法は、大きく分けて二種類存在します。&lt;/p&gt;

&lt;p&gt;まずひとつは、諦めることです。
コンフリクトが発生した時点でそれはLR(1)文法を逸脱しているため、もともと解析可能な構文ではありません。
構文規則を等価になるようにいろいろ書き換えるとうまくコンフリクトが消せる(かもしれない)ので、与える構文の見直しをします。&lt;/p&gt;

&lt;p&gt;もうひとつは、規則ごとにオペレーションの優先度を設定し、コンフリクトが発生した場合は強制的にどちらかの命令を実行すると決めてしまうことです。
これは一般的に行われている方法であり、かなり乱暴ですが大抵の場合はまあなんとかなります。&lt;/p&gt;

&lt;h2 id=&#34;参考資料:aebbcc1e0aae8e4a96be644adf00eb3f&#34;&gt;参考資料&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;で紹介したものを今回もそのまま参考資料としているため、基本的にはそちらをご覧ください。
今回紹介した内容の理解を深めるのに特に役立つと思われるおすすめの資料を抜粋しておきます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/courses/cs412/2003sp/lectures/lec09.pdf&#34;&gt;Cornell CIS Introduction to Compilers Lecture 9: LR(1) Parsing&lt;/a&gt;&lt;br /&gt;
LR(1) DFA、クロージャー展開、構文解析表等について詳細な定義や図解等が記載されています。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ichikaz3/lr-parsing&#34;&gt;LR parsing&lt;/a&gt;&lt;br /&gt;
クロージャー展開やDFAの構築、実際のオートマトンの動きに至るまで実際の動作過程を見ることができます。&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;今回でおおまかなLR(1)構文解析器作りの解説を済ませたので、次回からは実装をしていくだけです。
誰も他人のソースコードの解説なんて読む気は起きないでしょうし、これ以上続ける意味があるのか大いに疑問ではあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回:かんたん構文解析入門&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/&#34;&gt;前回:儀式の下準備&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第3回:儀式の下準備</title>
      <link>http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/</link>
      <pubDate>Tue, 21 Mar 2017 00:29:18 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;前回&lt;/a&gt;は字句解析器の作成を行ったので、次にLR(1)法による構文解析のためのパーサジェネレータの作成に入っていきます。
今回は、LR(1)構文解析器の構築のために必要な、終端記号と非終端記号の区別、Nulls集合、First集合の導出等を行えるようにしていきます。&lt;/p&gt;

&lt;p&gt;いよいよ構文解析部分の実装にとりかかるため、今後はLR(1)法に焦点を絞って解説していきます。&lt;/p&gt;

&lt;p&gt;今回はその準備段階として必要になる部分を作っていくため、どんどん実装を進めていきます。&lt;/p&gt;

&lt;h2 id=&#34;構文規則を定義する:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;構文規則を定義する&lt;/h2&gt;

&lt;p&gt;まずは、前回字句規則を定義したように、構文規則を定義していく必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;export interface LexDefinitionSection{
	token: Token|null;
	pattern: string|RegExp;
}
export type LexDefinitions = Array&amp;lt;LexDefinitionSection&amp;gt;;

export interface SyntaxDefinitionSection{
	ltoken: Token;
	pattern: Array&amp;lt;Token&amp;gt;;
}
export type SyntaxDefinitions = Array&amp;lt;SyntaxDefinitionSection&amp;gt;;

export interface GrammarDefinition{
	lex: LexDefinitions;
	syntax: SyntaxDefinitions;
	start_symbol: Token;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字句規則と構文規則を合わせて、上記のように定義しておきましょう。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;で定義した構文規則をこのデータ形式に直すと、以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;const syntax:SyntaxDefinitions = [
	{
		ltoken: &amp;quot;EXP&amp;quot;,
		pattern: [&amp;quot;EXP&amp;quot;, &amp;quot;PLUS&amp;quot;, &amp;quot;TERM&amp;quot;]
	},
	{
		ltoken: &amp;quot;EXP&amp;quot;,
		pattern: [&amp;quot;TERM&amp;quot;]
	},
	{
		ltoken: &amp;quot;TERM&amp;quot;,
		pattern: [&amp;quot;TERM&amp;quot;, &amp;quot;ASTERISK&amp;quot;, &amp;quot;ATOM&amp;quot;]
	},
	{
		ltoken: &amp;quot;TERM&amp;quot;,
		pattern: [&amp;quot;ATOM&amp;quot;]
	},
	{
		ltoken: &amp;quot;ATOM&amp;quot;,
		pattern:[&amp;quot;DIGITS&amp;quot;]
	},
	{
		ltoken: &amp;quot;ATOM&amp;quot;,
		pattern:[&amp;quot;LPAREN&amp;quot;, &amp;quot;EXP&amp;quot;, &amp;quot;RPAREN&amp;quot;]
	}
];
const lex:LexDefinitions = [
	{token:&amp;quot;DIGITS&amp;quot;, pattern:/[1-9][0-9]*/},
	{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, pattern:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, pattern:&amp;quot;(&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, pattern:&amp;quot;)&amp;quot;},
	{token:null, pattern:/\s/},
	{token:&amp;quot;INVALID&amp;quot;, pattern:/./},
];
const grammar:GrammarDefinition = {
	lex: lex,
	syntax: syntax,
	start_symbol: &amp;quot;EXP&amp;quot;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ルール一行ごとに一つのオブジェクトを割り当てています。
また、&lt;code&gt;start_symbol&lt;/code&gt;の定義を加えていることに注意してください。
これは文字通り開始記号のことであり、最初にどの記号から構文解析を開始するか表すために必要です。
最終的に構文解析を行った結果として構文木を得た場合、この開始記号が構文木の根となります。&lt;/p&gt;

&lt;h2 id=&#34;終端記号と非終端記号を区別する:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;終端記号と非終端記号を区別する&lt;/h2&gt;

&lt;p&gt;構文解析器を作るためには、与えられた構文から終端記号と非終端記号を区別できるようにする必要があります。
定義を再確認しておくと、規則の左辺に現れることのない記号が終端記号、現れる記号が非終端記号です。&lt;/p&gt;

&lt;p&gt;実装上難しい点は特にないので、簡単に済ませてしまいましょう。&lt;/p&gt;

&lt;p&gt;ひとまず、SymbolDiscriminatorというクラスを作って終端記号と非終端記号の問い合わせをできるようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// symboldiscriminator.d.ts
export declare class SymbolDiscriminator {
    private terminal_symbols;
    private nonterminal_symbols;
    constructor(syntaxdef: SyntaxDefinitions);
    getTerminalSymbols(): Set&amp;lt;Token&amp;gt;;
    getNonterminalSymbols(): Set&amp;lt;Token&amp;gt;;
    isTerminalSymbol(symbol: Token): boolean;
    isNonterminalSymbol(symbol: Token): boolean;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実際のコードは以下を参照してください。&lt;br /&gt;
&lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/symboldiscriminator.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/symboldiscriminator.ts&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// コンストラクタの実装のみ抜粋
constructor(syntaxdef:SyntaxDefinitions){
	this.terminal_symbols = new Set&amp;lt;Token&amp;gt;();
	this.nonterminal_symbols = new Set&amp;lt;Token&amp;gt;();

	// 左辺値の登録
	for(let sect of syntaxdef){
		let symbol = sect.ltoken;
		// 構文規則の左辺に現れる記号は非終端記号
		this.nonterminal_symbols.add(symbol);
	}
	// 右辺値の登録
	for(let sect of syntaxdef){
		for(let symbol of sect.pattern){
			if(!this.nonterminal_symbols.has(symbol)){
				// 非終端記号でない(=左辺値に現れない)場合、終端記号である
				this.terminal_symbols.add(symbol);
			}
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2つのSetを用意し、コンストラクタの呼び出し時に左辺に現れる記号とそうでない記号で分けています。
これで、トークンを与えるとそれが終端記号かどうか、非終端記号かどうかを判別できるようになりました。&lt;/p&gt;

&lt;p&gt;なおこの記事において、「トークン」と「記号」は概念上は同様の意味を持ちますが、前者はプログラム内で記号を表すための構造として扱い、理論的な概念について触れる際は後者を使うものとします。&lt;/p&gt;

&lt;h2 id=&#34;nulls集合とfirst集合:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合とFirst集合&lt;/h2&gt;

&lt;p&gt;構文解析器の作成のためには、First集合というものを求める必要があります。
そしてFisrt集合を求めるためには、Nulls集合が必要です。&lt;/p&gt;

&lt;p&gt;First集合はFirst関数などとも呼ばれます。
まあ名前なんてどうでもいいのですが、とにかくFirstとNullsを導出しなければなりません。
順を追って見て行きましょう。
例に漏れず、&lt;a href=&#34;https://twitter.com/ki6o4&#34;&gt;うさぎさん(@ki6o4)&lt;/a&gt;の&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が詳しいので厳密な定義等はそちらを参照してください(First,NullableはLL(1)の項で紹介されています)。&lt;/p&gt;

&lt;h3 id=&#34;nulls集合を求める:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合を求める&lt;/h3&gt;

&lt;h4 id=&#34;nulls集合とは:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合とは&lt;/h4&gt;

&lt;p&gt;ある終端記号または非終端記号について、それが「Nullableである」かどうかを判別する必要があります。
「Nullableな記号」を集めた集合をNulls集合ということにします。
記号がNullableであるとは、その記号から空列が導かれうることを意味します。&lt;/p&gt;

&lt;p&gt;今回題材としている数式の構文規則には、「右辺が存在しない」ルールはありません。
しかし、解析したい構文によっては、右辺が存在しない、つまり左辺から空列が導かれるルールが存在することがあります。
空列とはスペース(空白)等を意味するのではなく、長さ0の入力を意味します。
具体的には、次のようなルールを見てみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X -&amp;gt; Y &amp;quot;0&amp;quot;
Y -&amp;gt; &amp;quot;1&amp;quot;
Y -&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;構文規則の三行目の右辺には何も書かれていません。
このような場合、&lt;code&gt;Y&lt;/code&gt;は&lt;code&gt;1&lt;/code&gt;もしくは空列となり得ます。
よって&lt;code&gt;X&lt;/code&gt;は、&lt;code&gt;10&lt;/code&gt;と&lt;code&gt;0&lt;/code&gt;の2通りが許容されるのです。
ここで、&lt;code&gt;Y&lt;/code&gt;は空列となり得るため、Nulls集合に含まれます。
さらに、次のような例を見てください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Z -&amp;gt; Y
Y -&amp;gt; &amp;quot;1&amp;quot;
Y -&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この場合、&lt;code&gt;Z&lt;/code&gt;は&lt;code&gt;1&lt;/code&gt;と空列の2通りとなる可能性があります。
よって、&lt;code&gt;Z&lt;/code&gt;もNullableであるといえます。
このように、その記号自体が空列となるルールを指定していなくても、右辺の記号次第では空列となることがあります。
ひとつでも空列となるパターンが存在する場合、Nulls集合に含まなければなりません。
また当然ですが、左辺に現れることのない終端記号はNullableではありません。&lt;/p&gt;

&lt;h4 id=&#34;nulls集合を実装する:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合を実装する&lt;/h4&gt;

&lt;p&gt;なにやら面倒そうですが、実装はそう複雑ではありません。
NullableSetクラスを作ってみましょう。&lt;br /&gt;
&lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/nullableset.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/nullableset.ts&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// nullableset.ts
export class NullableSet{
	private nulls:Set&amp;lt;Token&amp;gt;;
	constructor(private syntax: SyntaxDefinitions){
		this.generateNulls();
	}
	// nulls初期化
	private generateNulls(){
		// 制約条件を導出するために、
		// 空列になりうる記号の集合nullsを導出
		this.nulls = new Set&amp;lt;Token&amp;gt;();
		for(let rule of this.syntax){
			// 右辺の記号の数が0の規則を持つ記号は空列になりうる
			if(rule.pattern.length == 0){
				this.nulls.add(rule.ltoken);
			}
		}

		// 変更が起きなくなるまでループする
		let flg_changed:boolean = true;
		while(flg_changed){
			flg_changed = false;
			for(let rule of this.syntax){
				// 既にnullsに含まれていればスキップ
				if(this.isNullable(rule.ltoken)) continue;

				let flg_nulls = true;
				// 右辺に含まれる記号がすべてnullableの場合はその左辺はnullable
				for(let token of rule.pattern){
					if(!this.isNullable(token)){
						// 一つでもnullableでない記号があるならnon-nullable
						flg_nulls = false;
						break;
					}
				}
				if(flg_nulls){
					flg_changed = true;
					this.nulls.add(rule.ltoken);
				}
			}
		}
	}
	public isNullable(x:Token){
		return this.nulls.has(x);
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;アルゴリズムは以下の通りです。&lt;br /&gt;
まず、右辺の記号の数が0になるような規則を持っている記号は明らかにNullableです。
そのため、初期化段階として、そのような規則を探してNulls集合に追加します。&lt;br /&gt;
次に、それ以外の記号がNullableであるためには、規則の右辺がすべてNullableな記号である必要があります。
そこで、すべての規則を調べ、その右辺の記号がすべて既存のNull集合に含まれているならば、その記号もNulls集合に追加します。&lt;br /&gt;
この処理を、一巡しても新しいNullableな規則が追加されなくなるまで繰り返せば終了です。&lt;/p&gt;

&lt;h3 id=&#34;first集合を求める:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;First集合を求める&lt;/h3&gt;

&lt;h4 id=&#34;first集合とは:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;First集合とは&lt;/h4&gt;

&lt;p&gt;たとえば先ほど挙げた&lt;code&gt;X&lt;/code&gt;が&lt;code&gt;10&lt;/code&gt;や&lt;code&gt;0&lt;/code&gt;となるように、非終端記号は規則をたどっていくと終端記号のみの列に変換することができます。
構文解析を行うためには、ある非終端記号から得られるそのような終端記号の列のうち、最も左側にどのような終端記号が来るのかを知る必要があります。
つまり(1)先読みを行うわけです(&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;参照)。&lt;/p&gt;

&lt;p&gt;このように、ある非終端記号と、そこから得られる可能性のある終端記号の列の先頭に来る記号の集合を対応付けたものを、First集合またはFirst関数と呼びます。
たとえば、先ほどの&lt;code&gt;X&lt;/code&gt;を例にすると、&lt;code&gt;First(X)&lt;/code&gt;は、&lt;code&gt;10&lt;/code&gt;と&lt;code&gt;0&lt;/code&gt;のそれぞれ左端の記号をとって&lt;code&gt;{1, 0}&lt;/code&gt;となります。&lt;/p&gt;

&lt;p&gt;Aが終端記号であるなら、&lt;code&gt;First(A)&lt;/code&gt;は&lt;code&gt;{A}&lt;/code&gt;(A自身のみを要素とする集合)です。
また、Firstに与える引数は記号だけでなく記号列である可能性もあります。
&lt;code&gt;First(YA)&lt;/code&gt;なら、&lt;code&gt;Y&lt;/code&gt;がNullableであるため、&lt;code&gt;YA&lt;/code&gt;から得られうる文字列の左端になりうるのは&lt;code&gt;1&lt;/code&gt;と&lt;code&gt;A&lt;/code&gt;なので、&lt;code&gt;{1, A}&lt;/code&gt;のようになるでしょう。&lt;/p&gt;

&lt;h4 id=&#34;first集合の実装:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;First集合の実装&lt;/h4&gt;

&lt;p&gt;話がごちゃごちゃしてきましたが、実装に移りましょう。
基本的なアルゴリズムは、以下のようになります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ある記号&lt;code&gt;A&lt;/code&gt;が終端記号なら、&lt;code&gt;First(A)&lt;/code&gt;は&lt;code&gt;{A}&lt;/code&gt;である。全ての終端記号についてそのように初期化する。&lt;/li&gt;
&lt;li&gt;非終端記号に対応するFirst集合は、まず空集合で初期化する。&lt;/li&gt;
&lt;li&gt;ルール &lt;code&gt;X -&amp;gt; Y1 Y2 ... Yi&lt;/code&gt; について、以下の制約を生成する。

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;First(X) ⊇ First(Y1)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y1&lt;/code&gt;がNullableなら &lt;code&gt;First(X) ⊇ First(Y2)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y1&lt;/code&gt;および&lt;code&gt;Y2&lt;/code&gt;がともにNullableなら &lt;code&gt;First(X) ⊇ First(Y3)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y1&lt;/code&gt;および&lt;code&gt;Y2&lt;/code&gt;および&lt;code&gt;Y3&lt;/code&gt;がともにNullableなら &lt;code&gt;First(X) ⊇ First(Y4)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;以下繰り返し&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;生成した制約に従い、スーパーセット側にサブセット側の集合の持つ記号を追加していく(制約の解消)。&lt;/li&gt;
&lt;li&gt;制約の解消を全ての集合に変化がなくなるまで繰り返す。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;わかりにくいですね。
ちなみにこのアルゴリズムは、言葉で定義するよりもプログラムを書いたほうがわかりやすい類のものです。&lt;/p&gt;

&lt;p&gt;とはいえちょっとコードが長いので、URLから参照をお願いします。
&lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/firstset.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/firstset.ts&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// firstset.d.ts
export declare class FirstSet {
    private syntax;
    private symbols;
    private first_map;
    private nulls;
    constructor(syntax: SyntaxDefinitions, symbols: SymbolDiscriminator);
    private generateFirst();
    get(arg: Token): Set&amp;lt;Token&amp;gt;;
    get(arg: Array&amp;lt;Token&amp;gt;): Set&amp;lt;Token&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実のところNulls集合はFirst集合を求める時にしか使わないので、First関数のprivateメンバとしてNullableSetインスタンスを生成して使用します。
SymbolDiscriminatorは他でも使いまわす必要があるので、コンストラクタ引数による依存性の注入を行います。&lt;/p&gt;

&lt;p&gt;また、First関数の引数には単一の記号だけでなく、記号列も与えられるようにする必要があります。
getメソッドではトークンを引数にとるだけでなく、トークンの配列も引数として与えることができるようにします。
トークンの配列が与えられた場合は、左から順に個別のトークンのFirst関数を呼び、そのトークンがNullableである限り、その右隣のFirst関数も呼び出し、その結果として得られた記号全てを要素とする集合を返すものとします。&lt;/p&gt;

&lt;p&gt;説明が適当かつわかりにくくて申し訳ありませんが、どうせ準備段階なので軽く飛ばして先に進めていきましょう(&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;に参考資料をまとめてあるため、詳細かつ厳密に知りたい方はそちらを参照してください)。&lt;/p&gt;

&lt;p&gt;これでひとまずFirst集合の導出まで終わったので、次回からは構文解析表の作成にとりかかります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回:かんたん構文解析入門&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;前回:字句解析器の実装&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://tatamo.81.la/blog/2017/03/22/lr-parser-generator-implementation-04/&#34;&gt;次回:かんたんLR(1)法入門&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第2回:字句解析器の実装</title>
      <link>http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/</link>
      <pubDate>Sat, 11 Feb 2017 18:13:48 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回&lt;/a&gt;は構文解析の概略を紹介したので、今回から実装に移っていきたいと思います。
まずは字句解析器を用意する必要があるため、今回は字句解析器の作成について紹介します。&lt;/p&gt;

&lt;p&gt;なお今回から実際のプログラムを記述していきますが、使用言語はTypeScriptとします。&lt;/p&gt;

&lt;p&gt;パーサジェネレータを作るのに比べれば字句解析器を作るのは非常に単純です。
早速はじめていきましょう。&lt;/p&gt;

&lt;h2 id=&#34;字句解析器の仕様を確認する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句解析器の仕様を確認する&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回&lt;/a&gt;の記事でも紹介しましたが、字句解析器の行う処理は以下のような流れになります。&lt;/p&gt;

&lt;p&gt;まず、解析するべき入力を文字列として受け取ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;9 + 11 * (2 + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これに加えて、字句規則を用意します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: /[1-9][0-9]*/
プラス: &amp;quot;+&amp;quot;
アステリスク: &amp;quot;*&amp;quot;
左括弧: &amp;quot;(&amp;quot;
右括弧: &amp;quot;)&amp;quot;
(読み捨て): /\s/
(不正): /./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字句解析器は受け取った入力を先頭から順に字句規則にあてはめ、マッチするものがあればそのトークンを割り当てます。
結果として得られる出力は、以下のようなリストになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: 9
プラス: +
数字: 11
アステリスク: *
左括弧: (
数字: 2
プラス: +
数字: 1
右括弧: )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;得られたトークンのリストを構文解析器の入力として渡すことで、構文解析器は文法の解析のみに注力することができます。&lt;/p&gt;

&lt;h2 id=&#34;字句規則を定義する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句規則を定義する&lt;/h2&gt;

&lt;p&gt;実際に解析を行うタイミングでは文字列のみを入力として受け取りますが、字句解析器の生成時には字句規則が必要です。
そのため、予め字句規則を別の設定ファイルなどに書いておくなどして用意しておかなければなりません。
ただし、&lt;strong&gt;字句規則の解析には構文解析器が必要&lt;/strong&gt;となるため、現時点ではプログラム内にハードコーディングしておくなどする必要があります。
今回は、字句規則を内部的に以下のようなデータ構造で扱うこととして、しばらくは字句規則をその内部データの形式で直接書くことにします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;// 定義
export type Token = string|symbol;
export interface LexDefinitionSection{
	token: Token|null;
	pattern: string|RegExp;
}
export type LexDefinitions = Array&amp;lt;LexDefinitionSection&amp;gt;;

// 実際の字句規則
const lex: LexDefinitions = [
	{token:&amp;quot;DIGITS&amp;quot;, pattern:/[1-9][0-9]*/},
	{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, pattern:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, pattern:&amp;quot;(&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, pattern:&amp;quot;)&amp;quot;},
	{token:null, pattern:/\s/},
	{token:&amp;quot;INVALID&amp;quot;, pattern:/./},
];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この字句規則の定義について、実用上の理由で追加したいくつかの仕様に注意する必要があります。&lt;/p&gt;

&lt;p&gt;Tokenの型定義にsymbolを含めている点についてはここで説明せずに後述することとします。&lt;/p&gt;

&lt;h3 id=&#34;入力の読み捨て:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;入力の読み捨て&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;{token:null, pattern:/\s/}
// (読み捨て): /\s/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この部分は、何らかの空白文字が入力に存在していればマッチングされます。
&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; を解析する際、ここに含まれている空白は文法上何の意味も持たず、&lt;code&gt;9+11*(2+1)&lt;/code&gt; のように入力が与えられたとしても解析結果は変化しません。
このような場合、構文解析器に空白の情報を与えることすらせずに、字句解析器上で空白を検知した段階でその情報を捨ててしまったほうが、構文解析器に余計な処理をさせずに済みます。&lt;/p&gt;

&lt;p&gt;今回は、トークンのラベル部分にnullを指定することで、読み取った結果をトークンとして保持することなく読み捨てることを表すようにしています。&lt;/p&gt;

&lt;h3 id=&#34;正規表現パターンと文字列パターンの使い分け:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;正規表現パターンと文字列パターンの使い分け&lt;/h3&gt;

&lt;p&gt;(ごちゃごちゃ書いている割に小手先のテクニックという感じが強いため、よくわからなければ読み飛ばしてください)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;}&lt;/code&gt; のように、パターン部分に正規表現ではなく文字列を用いて記述している箇所があります。
すべて正規表現を使って記述するのではなく文字列も許容している理由として、まず&lt;code&gt;/\+/&lt;/code&gt;のように特殊記号をエスケープせずに済む点が挙げられます。
そして、「文字列でパターンを記述した場合は、アルファベットの途中でトークンを区切らないようにする」というルールを用いることで、一部のパターンを簡潔に書くことが可能になります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vwxyz&lt;/code&gt; という入力を考えてみましょう。
ここでもし、&lt;code&gt;&amp;quot;vwx&amp;quot;&lt;/code&gt; にマッチする規則と、&lt;code&gt;&amp;quot;vwxyz&amp;quot;&lt;/code&gt; にマッチする規則の2つが存在した場合、&lt;code&gt;&amp;quot;vwxyz&amp;quot;&lt;/code&gt; の規則を先に書かない限り、入力&lt;code&gt;vwxyz&lt;/code&gt; は&lt;code&gt;&amp;quot;vwx&amp;quot; + yz&lt;/code&gt; とみなされ、&lt;code&gt;&amp;quot;yz&amp;quot;&lt;/code&gt; に対応する規則が存在しなければエラーとなります。
これを回避するためには、よりマッチするパターンが長い規則を常に短い規則よりも先に書くようにする必要がありますが、面倒です。
そこで、正規表現ではなく文字列で&lt;code&gt;&amp;quot;vwx&amp;quot;&lt;/code&gt; などのパターンが定義され、かつその末尾の文字が&lt;code&gt;\w&lt;/code&gt; にマッチする場合、マッチした部分の一文字先の文字が&lt;code&gt;\w&lt;/code&gt; 以外でなければマッチしないようにします。
これは、正規表現で&lt;code&gt;/vwx(?!\w)/&lt;/code&gt; 、&lt;code&gt;/vwxyz(?!\w)/&lt;/code&gt; というような否定的前方先読みをパターンの最後に追加することに相当します。
このルールを追加することで、正規表現を用いる場合よりも簡潔に記述可能となります。&lt;/p&gt;

&lt;h2 id=&#34;字句解析器を実装する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句解析器を実装する&lt;/h2&gt;

&lt;p&gt;ではLexerクラスを作っていきましょう。
とはいえ字句規則さえ定義してしまえば、やることはほとんどありません。
コンストラクタ引数として字句規則データを受け取って保持しておくようにして、解析実行時に上から順に字句規則のマッチングを試みるだけです。&lt;/p&gt;

&lt;p&gt;今回はコード量が少ないので、 &lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/lexer.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/lexer.ts&lt;/a&gt; 全体をそのまま貼り付けます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// token.ts
export type Token = string|symbol;
export const SYMBOL_EOF:Token = Symbol(&amp;quot;EOF&amp;quot;);
export const SYMBOL_SYNTAX:Token = Symbol(&amp;quot;S&#39;&amp;quot;);
export const SYMBOL_DOT:Token = Symbol(&amp;quot;.&amp;quot;);

export type TokenList = Array&amp;lt;{token:Token, value:string}&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// lexer.ts
/// LexDefinitionsの定義は先述のものと同一
import {Token, SYMBOL_EOF, TokenList} from &amp;quot;./token&amp;quot;;
import {LexDefinitions} from &amp;quot;./grammar&amp;quot;;

export interface ILexer{
	exec(str: string):TokenList;
}

export class Lexer implements ILexer{
	constructor(public def: LexDefinitions){
		// 正しいトークン定義が与えられているかチェック
		for(var i=0; i&amp;lt;this.def.length; i++){
			var token_pattern = this.def[i].pattern;
			if(typeof token_pattern == &amp;quot;string&amp;quot;){
				continue;
			}
			else if(token_pattern instanceof RegExp){
				// フラグを整形する
				let flags:string = &amp;quot;&amp;quot;;
				// gフラグは邪魔なので取り除く
				// i,m,uフラグがあれば維持する
				if(token_pattern.ignoreCase){
					flags += &amp;quot;i&amp;quot;;
				}
				if(token_pattern.multiline){
					flags += &amp;quot;m&amp;quot;;
				}
				if(token_pattern.unicode){
					flags += &amp;quot;u&amp;quot;;
				}
				// yフラグは必ずつける
				flags += &amp;quot;y&amp;quot;;
				// フラグをつけなおして新しい正規表現オブジェクトにする
				this.def[i].pattern = new RegExp(token_pattern, flags);
				continue;
			}
			throw new Error(&amp;quot;invalid token definition: neither string nor RegExp object&amp;quot;);
		}
	}
	exec(str: string):TokenList{
		var result:TokenList = [];
		let lastindex = 0;
		while(lastindex &amp;lt; str.length){
			for(var i=0; i&amp;lt;this.def.length; i++){
				var token:Token|null = this.def[i].token;
				var token_pattern = this.def[i].pattern;
				var match:string;
				if(typeof token_pattern == &amp;quot;string&amp;quot;){
					let last_tmp = lastindex+token_pattern.length;
					if(str.substring(lastindex,last_tmp) != token_pattern) continue;
					if(last_tmp &amp;lt; str.length &amp;amp;&amp;amp; /\w/.test(token_pattern.slice(-1)) &amp;amp;&amp;amp; /\w/.test(str[last_tmp])) continue; // ヒットした文字の末尾が\wで、そのすぐ後ろが\wの場合はスキップ
					match = token_pattern;
					lastindex += token_pattern.length;
				}
				else{
					// token_pattern: RegExp
					token_pattern.lastIndex = lastindex;
					let m = token_pattern.exec(str);
					if(m === null) continue; // マッチ失敗
					match = m[0];
					lastindex = token_pattern.lastIndex; // lastindexを進める
				}
				// tokenがnullなら処理を飛ばします
				if(token != null){
					result.push({token:token, value:match});
				}
				break;
			}
		}
		// 最後にEOFトークンを付与
		result.push({token:SYMBOL_EOF, value:&amp;quot;&amp;quot;});
		return result;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まずコンストラクタでは、与えられた字句規則に簡単な型チェックと正規表現の整形を行っています。
グローバルマッチは今回は邪魔なだけなので、与えられた正規表現にgフラグがついていれば取り除きます。
ES2015でRegExpに追加されたstickyフラグ(&lt;strong&gt;ほぼ&lt;/strong&gt;全ての主要モダンブラウザ上で実装済み)を使うと楽なので、ここでyフラグの追加も行います。&lt;/p&gt;

&lt;p&gt;execメソッドでは入力を読み終えるまでマッチングを繰り返し、&lt;code&gt;{token:Token, value:string}&lt;/code&gt; というオブジェクトを結果の配列に追加していきます。
先述のようにパターンが文字列であれば&lt;code&gt;\w&lt;/code&gt;が連続した場所では区切らないようにして、マッチングが成功するたびにインデックス位置を先に進めていきます。&lt;/p&gt;

&lt;p&gt;また、すべての入力を読み終えた後、最後にSymbol(EOF)を名前としたトークンを結果に追加します。
これは入力の末尾を意味するトークンで、構文解析の際に内部的に使用されます。&lt;/p&gt;

&lt;p&gt;(Symbolは、それ自身と比較しない限り&lt;code&gt;==&lt;/code&gt;や&lt;code&gt;===&lt;/code&gt;の評価結果が常にfalseになるプリミティブ型で、ES2015で追加されたものです。
字句規則で定義されたトークンとの衝突が発生しないようにここでSymbolを使用していますが、Symbolそのものはオブジェクトのプロパティとして使用することで後方互換性を維持することを目的としてJavaScriptに追加された型であるため、この用途で用いるのに適しているのかどうかは議論の余地があります。
とはいえプログラミング言語個別の問題はこの記事の主題とは関係がないため、詳しくは言及しません。)&lt;/p&gt;

&lt;p&gt;この字句解析器に先ほどの字句規則を与え、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; を入力すると、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;[
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;9&amp;quot;},
	{token:&amp;quot;PLUS&amp;quot;, value:&amp;quot;+&amp;quot;},
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;11&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, value:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, value:&amp;quot;(&amp;quot;},
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;2&amp;quot;},
	{token:&amp;quot;PLUS&amp;quot;, value:&amp;quot;+&amp;quot;},
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;1&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, value:&amp;quot;)&amp;quot;},
	{token:Symbol(EOF), value:&amp;quot;&amp;quot;}
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;という結果が得られます。
これでトークンの並びを得ることに成功したので、次回以降はいよいよパーサジェネレータの作成に移っていくことになります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回:かんたん構文解析入門&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/&#34;&gt;次回:儀式の下準備&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第1回:かんたん構文解析入門</title>
      <link>http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/</link>
      <pubDate>Thu, 22 Dec 2016 03:03:09 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/</guid>
      <description>
        

&lt;p&gt;この記事は&lt;a href=&#34;http://www.adventar.org/calendars/1881&#34;&gt;Kobe University Advent Calendar 2016&lt;/a&gt;の21日の記事です。また遅刻か。
なお私は当該大学の学部2年(2016年12月現在)です。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;構文解析ができるプログラマはちょっとかっこいいですよね。
「構文解析？ああ、できますよ」とか言って自分のスキルを自慢できそうな印象があります。&lt;/p&gt;

&lt;p&gt;(ほぼ)フルスクラッチでTypeScriptによるLR(1)パーサジェネレータを実装した(ついでにLALR(1)パーサも作れる)ので、これを完成させるまでの流れを紹介していこうと思います。&lt;/p&gt;

&lt;p&gt;今回は構文解析自体の入門編となります。&lt;/p&gt;

&lt;p&gt;自作したパーサジェネレータは &lt;a href=&#34;https://github.com/Tatamo/parsergenerator&#34;&gt;https://github.com/Tatamo/parsergenerator&lt;/a&gt; にあります。&lt;br /&gt;
今のところパーサジェネレータ部分は完成、基本的な構文解析なら問題なくこなせるので構文規則や字句規則を外部から読み取って構文解析してパーサジェネレータに渡すような処理や、全体の見通しを良くするための設計の見直しやリファクタリング等を行っている段階です。
ドキュメント作ってなくてすみません。&lt;/p&gt;

&lt;h2 id=&#34;構文解析をしたい:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析をしたい&lt;/h2&gt;

&lt;p&gt;構文解析、時々見かけるフレーズです。
プログラマなら覚えておいて損はない技術……かどうかはわかりませんが、そういう類のスキルに(傍からは)見えます。&lt;br /&gt;
ぜひやりましょう。&lt;/p&gt;

&lt;p&gt;ひとまず、何をやりたいのかを明確にする必要があります。&lt;br /&gt;
この記事では、「入力として与えられるLR(1)文法に属する文法に従ったトークン列をパース(構文解析)することで、その構造を構文木として出力する」ことを目標とします。
何を言っているのかさっぱりわかりませんね、わからなくていいです。&lt;/p&gt;

&lt;p&gt;順を追って説明する必要がありますが、詳細は適宜省略します。
そのため、まずは今回主に参照した資料を列挙しておきます。&lt;/p&gt;

&lt;h2 id=&#34;参考資料一覧:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;参考資料一覧&lt;/h2&gt;

&lt;p&gt;より詳しく知りたい方は、下記に挙げる資料やそこで紹介されている参考文献などを参照されるのが良いと思われます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/courses/cs412/2003sp/lectures/lec09.pdf&#34;&gt;Cornell CIS Introduction to Compilers Lecture 9: LR(1) Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jaist.ac.jp/~kshirai/lec/i223/04a.pdf&#34;&gt;JAIST 自然言語処理論Ｉ 4.文法2(構文解析) その1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jaist.ac.jp/~kshirai/lec/i223/04b.pdf&#34;&gt;JAIST 自然言語処理論Ｉ 4.文法2(構文解析) その2&lt;/a&gt;
(注：「LR法による構文解析」として紹介されているアルゴリズムはSLR法)&lt;br /&gt;
上記3つはネット上にアップロードされている特定の大学の講義資料ですが、公開の規定等を確認していないためリンクを張ることに不都合があるようなら知らせていただけると助かります。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_LR_parser&#34;&gt;Canonical LR parser - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ichikaz3/lr-parsing&#34;&gt;LR parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/uhyo_&#34;&gt;うひょ(@uhyo_)さん&lt;/a&gt; 生き字引。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;構文解析とは-ざっくり:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析とは？(ざっくり)&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; という数式を考えてみます。
構文解析をすることによる最終的な目的は、この数式を(たとえば)文字列として与えると、結果としてこの数式の答えが&lt;code&gt;42&lt;/code&gt;であることを導く、といったことです。&lt;/p&gt;

&lt;p&gt;そのためには、以下のものが必要になります：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数式を表現する構文規則&lt;/li&gt;
&lt;li&gt;上記構文規則を解析するように作られた構文解析器(Parser)&lt;/li&gt;
&lt;li&gt;解析された構文を処理するプログラム&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;さらに、これらの構文解析に入る前の下準備のために以下が必要です：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文字列をトークンとして分割して表現するための字句規則&lt;/li&gt;
&lt;li&gt;上記字句規則をもとに、文字列を読み取ってトークンを返す字句解析器(Lexical Analyzer、略してLexer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ちなみに、今回の記事の目標は、それらに加えて以下のものを実装することです：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;構文規則および字句規則を入力として与えることで、構文解析器そのものを自動生成するパーサジェネレータ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;実際の構文解析を行う手順とはずれてしまいますが、紹介した順番に沿って構文解析器→字句解析器の順に解説していきます。&lt;/p&gt;

&lt;h3 id=&#34;構文解析器-パーサ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析器(パーサ)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; という数式を解析するためには、まずこの数式がどのようなルールで記述されているのかを(再)定義する必要があります。
そのルールをを表すのが構文規則です。
構文規則を書き表すルールは、たとえば&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AB%E3%82%B9%E3%83%BB%E3%83%8A%E3%82%A6%E3%82%A2%E8%A8%98%E6%B3%95&#34;&gt;BNF&lt;/a&gt;など様々な種類がありますが、基本的な発想としては&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; X Y Z
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように左辺の記号を右辺の記号の並びによって定義することで行います。&lt;/p&gt;

&lt;p&gt;具体的に見てみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;式 -&amp;gt; 式 &amp;quot;+&amp;quot; 項
式 -&amp;gt; 項
項 -&amp;gt; 項 &amp;quot;*&amp;quot; 因子
項 -&amp;gt; 因子
因子 -&amp;gt; 数
因子 -&amp;gt; &amp;quot;(&amp;quot; 式 &amp;quot;)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;妥当ですね。
&lt;code&gt;式 -&amp;gt; 式 + 項&lt;/code&gt; と &lt;code&gt;式 -&amp;gt; 項&lt;/code&gt;の２つの規則が、再帰的な繰り返しを表現していることに注意してください。
たとえば、&lt;code&gt;項&lt;/code&gt;は当然&lt;code&gt;式&lt;/code&gt;ですし、&lt;code&gt;項 + 項&lt;/code&gt;も&lt;code&gt;式(-&amp;gt;項) + 項&lt;/code&gt; より&lt;code&gt;式&lt;/code&gt;となります。
さらに、&lt;code&gt;項 + 項 + 項&lt;/code&gt;は最初の&lt;code&gt;項 + 項&lt;/code&gt;が&lt;code&gt;式&lt;/code&gt;なので、&lt;code&gt;式(-&amp;gt;項 + 項) + 項&lt;/code&gt; より&lt;code&gt;式&lt;/code&gt;です。
よって、&lt;code&gt;式&lt;/code&gt;は&lt;code&gt;項&lt;/code&gt;を&lt;code&gt;&amp;quot;+&amp;quot;&lt;/code&gt;によって任意の回数だけ繋げたものであり、同様に&lt;code&gt;項&lt;/code&gt;は&lt;code&gt;因子&lt;/code&gt;を&lt;code&gt;&amp;quot;*&amp;quot;&lt;/code&gt;で繋げたものとなります。
最後に、&lt;code&gt;因子&lt;/code&gt;は単なる&lt;code&gt;数&lt;/code&gt;かもしれませんし、または&lt;code&gt;&amp;quot;(&amp;quot;&lt;/code&gt;と&lt;code&gt;&amp;quot;)&amp;quot;&lt;/code&gt;で囲まれた&lt;code&gt;式&lt;/code&gt;かもしれません。
これは括弧で囲まれた部分の式が他の部分よりも高い優先順位となることを表現しています。&lt;/p&gt;

&lt;p&gt;たとえば&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt;は、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;式{ [9] [+] [11 * (2 + 1)] }
式{ 項{ [9] } &amp;quot;+&amp;quot; 項{ [11] [*] [(2 + 1)] } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ [(] [2 + 1] [)] } } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ [2] [+] [1] } &amp;quot;)&amp;quot; } } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ 項{ [2] } &amp;quot;+&amp;quot; 項{ [1] } } &amp;quot;) &amp;quot;} } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ 項{ 因子{2} } &amp;quot;+&amp;quot; 項{ 因子{1} } } &amp;quot;)&amp;quot; } } }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように展開されます(こうして得られた構造をどう解析するかについては省略します)。この、解析対象→構文木の変換を自動で行うのがパーサです。&lt;/p&gt;

&lt;p&gt;ちなみにですが、この構文規則は解析したい対象ごとにあなたが一から書き上げる必要があります。&lt;/p&gt;

&lt;h3 id=&#34;字句解析器-レキシカルアナライザ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;字句解析器(レキシカルアナライザ)&lt;/h3&gt;

&lt;p&gt;上記構文規則では、&lt;code&gt;+&lt;/code&gt;や&lt;code&gt;*&lt;/code&gt;のような演算子、&lt;code&gt;数&lt;/code&gt;についての規定はありません。
これらの「左辺に現れない記号」を、「終端記号」と呼びます。左辺に現れる記号は非終端記号と呼ばれます。&lt;/p&gt;

&lt;p&gt;通常、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; のような入力は文字列で与えられますが、記号と記号の間には複数もしくは0個の空白が挿入されている可能性もあります。
しかし以下のような構文規則を定義するのは本質的ではありません：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;空白 -&amp;gt; &amp;quot; &amp;quot;
空白 -&amp;gt; &amp;quot; &amp;quot; 空白
数字 -&amp;gt; &amp;quot;0&amp;quot; | &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | ... | &amp;quot;9&amp;quot;
数字 -&amp;gt; (&amp;quot;0&amp;quot; | &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | ... | &amp;quot;9&amp;quot;) 数字
数 -&amp;gt; (&amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | &amp;quot;3&amp;quot; ... | &amp;quot;9&amp;quot;) 数字
ただし、|は「または」を表す
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;そこで、通常は「入力として与えられた文字列」を「終端記号として分類されたトークンの列」に変換する処理をはさみ、これによって得られたトークンを構文解析器に与えます。
トークンとは終端記号と、必要ならばそれに紐付いた元々の情報を保持しておいたものです。たとえば、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt;は、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: 9
プラス: +
数字: 11
アステリスク: *
左括弧: (
数字: 2
プラス: +
数字: 1
右括弧: )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というような9つのトークンの列に分けることができます。
構文解析器はそのトークンがどのような終端記号に対応しているかは見ますが、たとえば個々の数字が何であるかを判断することはしません。
これによって、構文解析器は本質的な文法の解析のみに注力することができます。&lt;/p&gt;

&lt;p&gt;この処理をするのが字句解析器で、どのような文字や文字列が与えられた場合に何という終端記号かを判別するための規則が字句規則です。&lt;/p&gt;

&lt;p&gt;字句規則は、例えば以下のような書き方になるでしょう：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: /[1-9][0-9]*/
プラス: &amp;quot;+&amp;quot;
アステリスク: &amp;quot;*&amp;quot;
左括弧: &amp;quot;(&amp;quot;
右括弧: &amp;quot;)&amp;quot;
(読み捨て): /\s/
(不正): /./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは字句規則の表現のために、文字列および正規表現を使用しています。
通常(?)字句規則は上から順に文字列の先頭部分を当てはめていき、マッチするものがあればその終端記号に対応付けます。
そのため、&lt;code&gt;(不正)&lt;/code&gt;の部分は入力された文字全てにマッチする正規表現&lt;code&gt;/./&lt;/code&gt;が使用されていますが、これは上の規則のいずれにも当てはまらなかった場合にのみマッチします。&lt;/p&gt;

&lt;p&gt;与えられた文字列を前から順番に見ていくだけなので、字句解析器の実装はパーサやパーサジェネレータの実装と比べると単純です。&lt;/p&gt;

&lt;h3 id=&#34;パーサジェネレータ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;パーサジェネレータ&lt;/h3&gt;

&lt;p&gt;ここまで構文解析器(パーサ)と字句解析器(レキシカルアナライザ)について見てきました。
基本的にはこの2つによって構文解析を行うことができ、基本的な流れとしては&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;入力となるような解析したい言語を用意する&lt;/li&gt;
&lt;li&gt;字句規則を用意して、それをもとにしたレキシカルアナライザを用意する&lt;/li&gt;
&lt;li&gt;レキシカルアナライザに入力を与え、トークンの列を取得する&lt;/li&gt;
&lt;li&gt;構文規則を用意して、それをもとにしたパーサを用意する&lt;/li&gt;
&lt;li&gt;パーサにトークンの列を与え、解析結果を得る&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;となります。
パーサジェネレータとは何かというと、この 4. の部分を自動化するものです。
(LR法の)構文解析器は、内部的には入力を受け取ってスタックに積みながら状態遷移を繰り返すオートマトンにすぎません。
そのため、どの入力が与えられればどのような状態に遷移するかを示す「構文解析表」を得ることができれば、その構文を解析するパーサを作成することができます。
パーサジェネレータは、構文規則を読み取ることでこの構文解析表をつくり上げるという処理を主に行います。&lt;/p&gt;

&lt;p&gt;字句解析器程度ならわざわざジェネレータを作らなくても、字句規則そのものを字句解析器に渡せば良い感じに字句解析してくれるようにできますが、パーサジェネレータも「構文解析表の構築後、それをもとにして構文解析を行う」ような機能がついていればそれはパーサであるとも言えます。
わざわざパーサとパーサジェネレータが分けられているのは、一つには計算資源の乏しかった昔はパーサジェネレータがオンメモリで展開した構文解析表をもとにそのままパーサとして振る舞うというようなことが少なく、構文解析表を与えることで「パーサのソースコード」を出力するようなものが一般的だったからではないかと思われます(適当な思いつきを言っています)。
もっとも、パーサジェネレータがパーサを生成する際の処理にかかる時間を省略したい場合、予めパーサをコンパイルしておけるようにするのは妥当といえるでしょう。
字句解析器のための「字句解析器ジェネレータ」も実際に存在していますが、ここでは簡単のために字句解析器はコンストラクタに字句規則を与えれば勝手に良い感じの字句解析を行ってくれるようになるものと思ってもらえればよいです。&lt;/p&gt;

&lt;h3 id=&#34;文脈自由言語について:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;文脈自由言語について&lt;/h3&gt;

&lt;p&gt;構文解析器が解析対象とする「言語」がどのようなものであるかについてはいろいろな定義がなされています。&lt;/p&gt;

&lt;p&gt;これについては、参考資料でも紹介した&lt;a href=&#34;https://twitter.com/ki6o4&#34;&gt;うさぎさん(@ki6o4)&lt;/a&gt;の&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が詳しいため、こちらを参照していただくことをおすすめします。
ここでは、厳密な話はあまりせずにごくごく簡単に触れていこうと思います。&lt;/p&gt;

&lt;p&gt;構文解析の対象とするのは、基本的に文脈自由言語となります。
構文解析の手法にも様々なものがありますが、それらの手法の中には文脈自由言語すべてを解析できるわけではないものも多く、たとえばLR(1)法ならLR(1)文法やLR(1)言語というように、ある手法で解析できる文法や、解析できる言語全体をその手法の名前で表される言語として表現することがあります。&lt;/p&gt;

&lt;h3 id=&#34;解析手法と言語のクラス:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;解析手法と言語のクラス&lt;/h3&gt;

&lt;p&gt;いくつかの手法を主観を交えて乱暴に紹介していきます。&lt;/p&gt;

&lt;h4 id=&#34;先読み:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;先読み&lt;/h4&gt;

&lt;p&gt;**この項は下のLR法などの項を「先読み」してから戻ってきて読むことをおすすめします**&lt;/p&gt;

&lt;p&gt;たとえばLR(1)法のように、数字を括弧でくくって(k)と表現している手法がいくつかあります。このkは何文字先読みするかを示していて、たとえば(1)ならば1文字先読みするという意味です。
先読み数については、たとえばLR法については以下のようなことが言われています。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;LR(k)で表せる文法のクラス ⊆ LR(k+1)先読みで表せるクラス である&lt;/li&gt;
&lt;li&gt;LR(k)文法によって受理可能な言語のクラスは、LR(1)のそれと等しい&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2.より、基本的には(1)について考えることが多いようです。&lt;/p&gt;

&lt;h4 id=&#34;ll-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LL(1)法&lt;/h4&gt;

&lt;p&gt;「再帰を使って構文解析する」という発想としては単純なもの。
LL(1)文法のクラスはLR(1)よりも大幅に小さいものの、それでもLALR(1)文法を外れた文法を解析できたりします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; S + E
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というような、右辺の一番左の場所に左辺の記号が登場するような「左再帰則」を読むことができません。ナンセンス。&lt;/p&gt;

&lt;h4 id=&#34;lr-0-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(0)法&lt;/h4&gt;

&lt;p&gt;先読み数が0なのでよわい。&lt;/p&gt;

&lt;h4 id=&#34;slr法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;SLR法&lt;/h4&gt;

&lt;p&gt;SLRのSはSimpleの意味です。LR(0)から単純な先読みを加えることでLR(0)よりも解析可能な文法が増えますが、それでもLALR(1)には及びません。&lt;/p&gt;

&lt;h4 id=&#34;lr-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(1)法&lt;/h4&gt;

&lt;p&gt;LR(0)に対し、1文字だけ先読みして次にどのような入力が期待されるかを判断。
LR(1)文法がそれなりに広いという点で優秀な一方、LR(0)に比べて構文解析表の大きさが爆発しやすいという欠点がある、と言われています。
しかし今の時代はそんなものは大した欠点になり得ない気がします。&lt;/p&gt;

&lt;h4 id=&#34;lalr-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LALR(1)法&lt;/h4&gt;

&lt;p&gt;プログラミング言語を解析するコンパイラなどによく使われている手法です。
LALR(1)のLAはLook-Aheadの略で、まずLR(1)法で構文解析表を作ってから、文法部分が同じで先読み記号だけが違うような状態をマージするという点がLR(1)法と異なります。
表を併合してしまうためにLR(1)法よりも解析可能な文法のクラスが小さくなるものの、実用上はほとんど問題にならず、LR(1)法の構文解析表が大きくなりすぎるという欠点を補える手法です。&lt;/p&gt;

&lt;p&gt;ただし、一度LR(1)法の表を作ること変わりはないのでメモリ消費量はそう変わらないし、大きなデータも問題なく扱える今の時代にわざわざ構文解析表を数十パーセント程度削減したところで何の意味があるのかという疑問があります。&lt;/p&gt;

&lt;p&gt;また、LALR法のLAはLook-Aheadの略だと言いましたが、注意しなければならないのは&lt;strong&gt;Look-Ahead(先読み)を行うのはLALR法固有の手法ではない&lt;/strong&gt;ということです。  先読み自体はLR(1)法でもやりますし、LALR(1)はあくまでLR(1)の先読み部分をマージしたものにすぎません。
私はLALR法の名前の付け方はあまり良くないと思っていて、MLR法(Merged Look-Ahead LR法)とかなんとか、そういう感じの名前に変えたほうが良いと思います。&lt;/p&gt;

&lt;h4 id=&#34;glr法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;GLR法&lt;/h4&gt;

&lt;p&gt;「あいまいな」解釈が可能な文法があった場合、考えられうるすべての可能性を探索してしまうことによって解決する手法。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;「お魚くわえた猫を追いかけるサザエさん」&lt;/code&gt;で魚をくわえているのが猫とサザエさんの両方に解釈できるように、一つの入力に対して複数の結果が得られることがあります。
どちらかというと自然言語処理向きかもしれません。&lt;/p&gt;

&lt;h4 id=&#34;cyk法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;CYK法&lt;/h4&gt;

&lt;p&gt;強力なアルゴリズムにより、文脈自由言語すべてを比較的高速に解析可能。
ただし、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; NP VP
VP -&amp;gt; v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように、チョムスキー標準形といわれるような、「右辺が非終端記号ちょうど2つか終端記号1つでなければならない」、つまり結果として得られる構造が二分木になっていなければならないというナンセンスにも程がある制約を課されます(アルゴリズムの改良や規則の変換、得られた木構造の後処理などによって回避は可能ですが)。&lt;/p&gt;

&lt;p&gt;このCYK法の計算量オーダーは&lt;code&gt;O(n^3)&lt;/code&gt;程度で、文脈自由言語全てを解析可能なアルゴリズムの中では高速ですが、この記事で紹介されている他のアルゴリズムよりは低速となります。
たとえばLR(1)法は文脈自由言語全体を解析出来ないかわりに&lt;code&gt;O(n)&lt;/code&gt;で解析が可能です。&lt;/p&gt;

&lt;p&gt;プログラミング言語の解析では、言語の開発者が文法自体をある程度自由に定義することができるため、文脈自由言語の一部だけでなく全体を解析したいという需要はあまり発生しません。&lt;/p&gt;

&lt;h2 id=&#34;lr-1-パーサジェネレータをつくろう:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(1)パーサジェネレータをつくろう&lt;/h2&gt;

&lt;p&gt;構文解析の大まかな流れはわかりました。
とりあえず字句規則と構文規則を用意して、あとはどうにかしてこの構文を読んでくれるようなパーサを用意すれば構文解析ができそうです(字句解析器なんてのは適当にやってもすぐ用意できます)。&lt;/p&gt;

&lt;p&gt;先ほど紹介したような手法によって構文ごとに一からパーサをプログラミングするようなことはやりたくないので、パーサジェネレータを用いてパーサを自動的に生成してもらえば事は済みそうですね。&lt;/p&gt;

&lt;p&gt;ここに&lt;a href=&#34;https://www.gnu.org/software/bison/&#34;&gt;Bison&lt;/a&gt;という有名なパーサジェネレータがあります。
今の時代にわざわざCやC++で構文解析なんてしたくないのでしたら、Pythonで&lt;a href=&#34;http://www.dabeaz.com/ply/&#34;&gt;PLY&lt;/a&gt;とか、JavaScriptの&lt;a href=&#34;https://github.com/zaach/jison&#34;&gt;jison&lt;/a&gt;というものなど、いくらでも選択肢があります。
これらのうち一つを選んで、チュートリアルを読んでパーサを作っていくのがいいでしょう。&lt;/p&gt;

&lt;p&gt;というわけで、前段を書いていると結構分量が膨らんでしまったため、今回はここで区切ります。&lt;/p&gt;

&lt;p&gt;では次回からは、構文解析を行えるようになるため、LR(1)法を用いたパーサジェネレータを実際に作っていく流れを紹介していきたいと思います。&lt;/p&gt;

&lt;p&gt;えっちょっとまって、今パーサジェネレータは既存のものを使えばいいって言ったよね、ねえ&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;次回:字句解析器の実装&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>