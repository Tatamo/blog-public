<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Parsing on わたしろぐ</title>
    <link>http://tatamo.81.la/blog/tags/parsing/</link>
    <description>Recent content in Parsing on わたしろぐ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Tue, 21 Mar 2017 00:29:18 +0900</lastBuildDate>
    <atom:link href="http://tatamo.81.la/blog/tags/parsing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第3回:儀式の下準備</title>
      <link>http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/</link>
      <pubDate>Tue, 21 Mar 2017 00:29:18 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;前回&lt;/a&gt;は字句解析器の作成を行ったので、次にLR(1)法による構文解析のためのパーサジェネレータの作成に入っていきます。
今回は、LR(1)構文解析器の構築のために必要な、終端記号と非終端記号の区別、Nulls集合、First集合の導出等を行えるようにしていきます。&lt;/p&gt;

&lt;p&gt;いよいよ構文解析部分の実装にとりかかるため、今後はLR(1)法に焦点を絞って解説していきます。&lt;/p&gt;

&lt;p&gt;今回はその準備段階として必要になる部分を作っていくため、どんどん実装を進めていきます。&lt;/p&gt;

&lt;h2 id=&#34;構文規則を定義する:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;構文規則を定義する&lt;/h2&gt;

&lt;p&gt;まずは、前回字句規則を定義したように、構文規則を定義していく必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;export interface LexDefinitionSection{
	token: Token|null;
	pattern: string|RegExp;
}
export type LexDefinitions = Array&amp;lt;LexDefinitionSection&amp;gt;;

export interface SyntaxDefinitionSection{
	ltoken: Token;
	pattern: Array&amp;lt;Token&amp;gt;;
}
export type SyntaxDefinitions = Array&amp;lt;SyntaxDefinitionSection&amp;gt;;

export interface GrammarDefinition{
	lex: LexDefinitions;
	syntax: SyntaxDefinitions;
	start_symbol: Token;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字句規則と構文規則を合わせて、上記のように定義しておきましょう。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;で定義した構文規則をこのデータ形式に直すと、以下のようになります。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;const syntax:SyntaxDefinitions = [
	{
		ltoken: &amp;quot;EXP&amp;quot;,
		pattern: [&amp;quot;EXP&amp;quot;, &amp;quot;PLUS&amp;quot;, &amp;quot;TERM&amp;quot;]
	},
	{
		ltoken: &amp;quot;EXP&amp;quot;,
		pattern: [&amp;quot;TERM&amp;quot;]
	},
	{
		ltoken: &amp;quot;TERM&amp;quot;,
		pattern: [&amp;quot;TERM&amp;quot;, &amp;quot;ASTERISK&amp;quot;, &amp;quot;ATOM&amp;quot;]
	},
	{
		ltoken: &amp;quot;TERM&amp;quot;,
		pattern: [&amp;quot;ATOM&amp;quot;]
	},
	{
		ltoken: &amp;quot;ATOM&amp;quot;,
		pattern:[&amp;quot;DIGITS&amp;quot;]
	},
	{
		ltoken: &amp;quot;ATOM&amp;quot;,
		pattern:[&amp;quot;LPAREN&amp;quot;, &amp;quot;EXP&amp;quot;, &amp;quot;RPAREN&amp;quot;]
	}
];
const lex:LexDefinitions = [
	{token:&amp;quot;DIGITS&amp;quot;, pattern:/[1-9][0-9]*/},
	{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, pattern:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, pattern:&amp;quot;(&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, pattern:&amp;quot;)&amp;quot;},
	{token:null, pattern:/\s/},
	{token:&amp;quot;INVALID&amp;quot;, pattern:/./},
];
const grammar:GrammarDefinition = {
	lex: lex,
	syntax: syntax,
	start_symbol: &amp;quot;EXP&amp;quot;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ルール一行ごとに一つのオブジェクトを割り当てています。
また、&lt;code&gt;start_symbol&lt;/code&gt;の定義を加えていることに注意してください。
これは文字通り開始記号のことであり、最初にどの記号から構文解析を開始するか表すために必要です。
最終的に構文解析を行った結果として構文木を得た場合、この開始記号が構文木の根となります。&lt;/p&gt;

&lt;h2 id=&#34;終端記号と非終端記号を区別する:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;終端記号と非終端記号を区別する&lt;/h2&gt;

&lt;p&gt;構文解析器を作るためには、与えられた構文から終端記号と非終端記号を区別できるようにする必要があります。
定義を再確認しておくと、規則の左辺に現れることのない記号が終端記号、現れる記号が非終端記号です。&lt;/p&gt;

&lt;p&gt;実装上難しい点は特にないので、簡単に済ませてしまいましょう。&lt;/p&gt;

&lt;p&gt;ひとまず、SymbolDiscriminatorというクラスを作って終端記号と非終端記号の問い合わせをできるようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// symboldiscriminator.d.ts
export declare class SymbolDiscriminator {
    private terminal_symbols;
    private nonterminal_symbols;
    constructor(syntaxdef: SyntaxDefinitions);
    getTerminalSymbols(): Set&amp;lt;Token&amp;gt;;
    getNonterminalSymbols(): Set&amp;lt;Token&amp;gt;;
    isTerminalSymbol(symbol: Token): boolean;
    isNonterminalSymbol(symbol: Token): boolean;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実際のコードは以下を参照してください。&lt;br /&gt;
&lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/symboldiscriminator.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/symboldiscriminator.ts&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// コンストラクタの実装のみ抜粋
constructor(syntaxdef:SyntaxDefinitions){
	this.terminal_symbols = new Set&amp;lt;Token&amp;gt;();
	this.nonterminal_symbols = new Set&amp;lt;Token&amp;gt;();

	// 左辺値の登録
	for(let sect of syntaxdef){
		let symbol = sect.ltoken;
		// 構文規則の左辺に現れる記号は非終端記号
		this.nonterminal_symbols.add(symbol);
	}
	// 右辺値の登録
	for(let sect of syntaxdef){
		for(let symbol of sect.pattern){
			if(!this.nonterminal_symbols.has(symbol)){
				// 非終端記号でない(=左辺値に現れない)場合、終端記号である
				this.terminal_symbols.add(symbol);
			}
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2つのSetを用意し、コンストラクタの呼び出し時に左辺に現れる記号とそうでない記号で分けています。
これで、トークンを与えるとそれが終端記号かどうか、非終端記号かどうかを判別できるようになりました。&lt;/p&gt;

&lt;p&gt;なおこの記事において、「トークン」と「記号」は概念上は同様の意味を持ちますが、前者はプログラム内で記号を表すための構造として扱い、理論的な概念について触れる際は後者を使うものとします。&lt;/p&gt;

&lt;h2 id=&#34;nulls集合とfirst集合:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合とFirst集合&lt;/h2&gt;

&lt;p&gt;構文解析器の作成のためには、First集合というものを求める必要があります。
そしてFisrt集合を求めるためには、Nulls集合が必要です。&lt;/p&gt;

&lt;p&gt;First集合はFirst関数などとも呼ばれます。
まあ名前なんてどうでもいいのですが、とにかくFirstとNullsを導出しなければなりません。
順を追って見て行きましょう。
例に漏れず、&lt;a href=&#34;https://twitter.com/ki6o4&#34;&gt;うさぎさん(@ki6o4)&lt;/a&gt;の&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が詳しいので厳密な定義等はそちらを参照してください(First,NullableはLL(1)の項で紹介されています)。&lt;/p&gt;

&lt;h3 id=&#34;nulls集合を求める:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合を求める&lt;/h3&gt;

&lt;h4 id=&#34;nulls集合とは:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合とは&lt;/h4&gt;

&lt;p&gt;ある終端記号または非終端記号について、それが「Nullableである」かどうかを判別する必要があります。
「Nullableな記号」を集めた集合をNulls集合ということにします。
記号がNullableであるとは、その記号から空列が導かれうることを意味します。&lt;/p&gt;

&lt;p&gt;今回題材としている数式の構文規則には、「右辺が存在しない」ルールはありません。
しかし、解析したい構文によっては、右辺が存在しない、つまり左辺から空列が導かれるルールが存在することがあります。
空列とはスペース(空白)等を意味するのではなく、長さ0の入力を意味します。
具体的には、次のようなルールを見てみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;X -&amp;gt; Y &amp;quot;0&amp;quot;
Y -&amp;gt; &amp;quot;1&amp;quot;
Y -&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;構文規則の三行目の右辺には何も書かれていません。
このような場合、&lt;code&gt;Y&lt;/code&gt;は&lt;code&gt;1&lt;/code&gt;もしくは空列となり得ます。
よって&lt;code&gt;X&lt;/code&gt;は、&lt;code&gt;10&lt;/code&gt;と&lt;code&gt;0&lt;/code&gt;の2通りが許容されるのです。
ここで、&lt;code&gt;Y&lt;/code&gt;は空列となり得るため、Nulls集合に含まれます。
さらに、次のような例を見てください。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Z -&amp;gt; Y
Y -&amp;gt; &amp;quot;1&amp;quot;
Y -&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この場合、&lt;code&gt;Z&lt;/code&gt;は&lt;code&gt;1&lt;/code&gt;と空列の2通りとなる可能性があります。
よって、&lt;code&gt;Z&lt;/code&gt;もNullableであるといえます。
このように、その記号自体が空列となるルールを指定していなくても、右辺の記号次第では空列となることがあります。
ひとつでも空列となるパターンが存在する場合、Nulls集合に含まなければなりません。
また当然ですが、左辺に現れることのない終端記号はNullableではありません。&lt;/p&gt;

&lt;h4 id=&#34;nulls集合を実装する:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;Nulls集合を実装する&lt;/h4&gt;

&lt;p&gt;なにやら面倒そうですが、実装はそう複雑ではありません。
NullableSetクラスを作ってみましょう。&lt;br /&gt;
&lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/nullableset.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/nullableset.ts&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// nullableset.ts
export class NullableSet{
	private nulls:Set&amp;lt;Token&amp;gt;;
	constructor(private syntax: SyntaxDefinitions){
		this.generateNulls();
	}
	// nulls初期化
	private generateNulls(){
		// 制約条件を導出するために、
		// 空列になりうる記号の集合nullsを導出
		this.nulls = new Set&amp;lt;Token&amp;gt;();
		for(let rule of this.syntax){
			// 右辺の記号の数が0の規則を持つ記号は空列になりうる
			if(rule.pattern.length == 0){
				this.nulls.add(rule.ltoken);
			}
		}

		// 変更が起きなくなるまでループする
		let flg_changed:boolean = true;
		while(flg_changed){
			flg_changed = false;
			for(let rule of this.syntax){
				// 既にnullsに含まれていればスキップ
				if(this.isNullable(rule.ltoken)) continue;

				let flg_nulls = true;
				// 右辺に含まれる記号がすべてnullableの場合はその左辺はnullable
				for(let token of rule.pattern){
					if(!this.isNullable(token)){
						// 一つでもnullableでない記号があるならnon-nullable
						flg_nulls = false;
						break;
					}
				}
				if(flg_nulls){
					flg_changed = true;
					this.nulls.add(rule.ltoken);
				}
			}
		}
	}
	public isNullable(x:Token){
		return this.nulls.has(x);
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;アルゴリズムは以下の通りです。&lt;br /&gt;
まず、右辺の記号の数が0になるような規則を持っている記号は明らかにNullableです。
そのため、初期化段階として、そのような規則を探してNulls集合に追加します。&lt;br /&gt;
次に、それ以外の記号がNullableであるためには、規則の右辺がすべてNullableな記号である必要があります。
そこで、すべての規則を調べ、その右辺の記号がすべて既存のNull集合に含まれているならば、その記号もNulls集合に追加します。&lt;br /&gt;
この処理を、一巡しても新しいNullableな規則が追加されなくなるまで繰り返せば終了です。&lt;/p&gt;

&lt;h3 id=&#34;first集合を求める:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;First集合を求める&lt;/h3&gt;

&lt;h4 id=&#34;first集合とは:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;First集合とは&lt;/h4&gt;

&lt;p&gt;たとえば先ほど挙げた&lt;code&gt;X&lt;/code&gt;が&lt;code&gt;10&lt;/code&gt;や&lt;code&gt;0&lt;/code&gt;となるように、非終端記号は規則をたどっていくと終端記号のみの列に変換することができます。
構文解析を行うためには、ある非終端記号から得られるそのような終端記号の列のうち、最も左側にどのような終端記号が来るのかを知る必要があります。
つまり(1)先読みを行うわけです(&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;参照)。&lt;/p&gt;

&lt;p&gt;このように、ある非終端記号と、そこから得られる可能性のある終端記号の列の先頭に来る記号の集合を対応付けたものを、First集合またはFirst関数と呼びます。
たとえば、先ほどの&lt;code&gt;X&lt;/code&gt;を例にすると、&lt;code&gt;First(X)&lt;/code&gt;は、&lt;code&gt;10&lt;/code&gt;と&lt;code&gt;0&lt;/code&gt;のそれぞれ左端の記号をとって&lt;code&gt;{1, 0}&lt;/code&gt;となります。&lt;/p&gt;

&lt;p&gt;Aが終端記号であるなら、&lt;code&gt;First(A)&lt;/code&gt;は&lt;code&gt;{A}&lt;/code&gt;(A自身のみを要素とする集合)です。
また、Firstに与える引数は記号だけでなく記号列である可能性もあります。
&lt;code&gt;First(YA)&lt;/code&gt;なら、&lt;code&gt;Y&lt;/code&gt;がNullableであるため、&lt;code&gt;YA&lt;/code&gt;から得られうる文字列の左端になりうるのは&lt;code&gt;1&lt;/code&gt;と&lt;code&gt;A&lt;/code&gt;なので、&lt;code&gt;{1, A}&lt;/code&gt;のようになるでしょう。&lt;/p&gt;

&lt;h4 id=&#34;first集合の実装:54819cf28b4a2703da41ec17803c8ec7&#34;&gt;First集合の実装&lt;/h4&gt;

&lt;p&gt;話がごちゃごちゃしてきましたが、実装に移りましょう。
基本的なアルゴリズムは、以下のようになります。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ある記号&lt;code&gt;A&lt;/code&gt;が終端記号なら、&lt;code&gt;First(A)&lt;/code&gt;は&lt;code&gt;{A}&lt;/code&gt;である。全ての終端記号についてそのように初期化する。&lt;/li&gt;
&lt;li&gt;非終端記号に対応するFirst集合は、まず空集合で初期化する。&lt;/li&gt;
&lt;li&gt;ルール &lt;code&gt;X -&amp;gt; Y1 Y2 ... Yi&lt;/code&gt; について、以下の制約を生成する。

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;First(X) ⊇ First(Y1)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y1&lt;/code&gt;がNullableなら &lt;code&gt;First(X) ⊇ First(Y2)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Y2&lt;/code&gt;がNullableなら &lt;code&gt;First(X) ⊇ First(Y3)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;以下繰り返し&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;生成した制約に従い、スーパーセット側にサブセット側の集合の持つ記号を追加していく(制約の解消)。&lt;/li&gt;
&lt;li&gt;制約の解消を全ての集合に変化がなくなるまで繰り返す。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;わかりにくいですね。
ちなみにこのアルゴリズムは、言葉で定義するよりもプログラムを書いたほうがわかりやすい類のものです。&lt;/p&gt;

&lt;p&gt;とはいえちょっとコードが長いので、URLから参照をお願いします。
&lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/firstset.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/parsergenerator/firstset.ts&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// firstset.d.ts
export declare class FirstSet {
    private syntax;
    private symbols;
    private first_map;
    private nulls;
    constructor(syntax: SyntaxDefinitions, symbols: SymbolDiscriminator);
    private generateFirst();
    get(arg: Token): Set&amp;lt;Token&amp;gt;;
    get(arg: Array&amp;lt;Token&amp;gt;): Set&amp;lt;Token&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実のところNulls集合はFirst集合を求める時にしか使わないので、First関数のprivateメンバとしてNullableSetインスタンスを生成して使用します。
SymbolDiscriminatorは他でも使いまわす必要があるので、コンストラクタ引数による依存性の注入を行います。&lt;/p&gt;

&lt;p&gt;また、First関数の引数には単一の記号だけでなく、記号列も与えられるようにする必要があります。
getメソッドではトークンを引数にとるだけでなく、トークンの配列も引数として与えることができるようにします。
トークンの配列が与えられた場合は、左から順に個別のトークンのFirst関数を呼び、そのトークンがNullableである限り、その右隣のFirst関数も呼び出し、その結果として得られた記号全てを要素とする集合を返すものとします。&lt;/p&gt;

&lt;p&gt;説明が適当かつわかりにくくて申し訳ありませんが、どうせ準備段階なので軽く飛ばして先に進めていきましょう(&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回&lt;/a&gt;に参考資料をまとめてあるため、詳細かつ厳密に知りたい方はそちらを参照してください)。&lt;/p&gt;

&lt;p&gt;これでひとまずFirst集合の導出まで終わったので、次回からは構文解析表の作成にとりかかります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;第1回:かんたん構文解析入門&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;前回:字句解析器の実装&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第2回:字句解析器の実装</title>
      <link>http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/</link>
      <pubDate>Sat, 11 Feb 2017 18:13:48 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/</guid>
      <description>
        

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回&lt;/a&gt;は構文解析の概略を紹介したので、今回から実装に移っていきたいと思います。
まずは字句解析器を用意する必要があるため、今回は字句解析器の作成について紹介します。&lt;/p&gt;

&lt;p&gt;なお今回から実際のプログラムを記述していきますが、使用言語はTypeScriptとします。&lt;/p&gt;

&lt;p&gt;パーサジェネレータを作るのに比べれば字句解析器を作るのは非常に単純です。
早速はじめていきましょう。&lt;/p&gt;

&lt;h2 id=&#34;字句解析器の仕様を確認する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句解析器の仕様を確認する&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回&lt;/a&gt;の記事でも紹介しましたが、字句解析器の行う処理は以下のような流れになります。&lt;/p&gt;

&lt;p&gt;まず、解析するべき入力を文字列として受け取ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;9 + 11 * (2 + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これに加えて、字句規則を用意します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: /[1-9][0-9]*/
プラス: &amp;quot;+&amp;quot;
アステリスク: &amp;quot;*&amp;quot;
左括弧: &amp;quot;(&amp;quot;
右括弧: &amp;quot;)&amp;quot;
(読み捨て): /\s/
(不正): /./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;字句解析器は受け取った入力を先頭から順に字句規則にあてはめ、マッチするものがあればそのトークンを割り当てます。
結果として得られる出力は、以下のようなリストになります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: 9
プラス: +
数字: 11
アステリスク: *
左括弧: (
数字: 2
プラス: +
数字: 1
右括弧: )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;得られたトークンのリストを構文解析器の入力として渡すことで、構文解析器は文法の解析のみに注力することができます。&lt;/p&gt;

&lt;h2 id=&#34;字句規則を定義する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句規則を定義する&lt;/h2&gt;

&lt;p&gt;実際に解析を行うタイミングでは文字列のみを入力として受け取りますが、字句解析器の生成時には字句規則が必要です。
そのため、予め字句規則を別の設定ファイルなどに書いておくなどして用意しておかなければなりません。
ただし、&lt;strong&gt;字句規則の解析には構文解析器が必要&lt;/strong&gt;となるため、現時点ではプログラム内にハードコーディングしておくなどする必要があります。
今回は、字句規則を内部的に以下のようなデータ構造で扱うこととして、しばらくは字句規則をその内部データの形式で直接書くことにします。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;// 定義
export type Token = string|symbol;
export interface LexDefinitionSection{
	token: Token|null;
	pattern: string|RegExp;
}
export type LexDefinitions = Array&amp;lt;LexDefinitionSection&amp;gt;;

// 実際の字句規則
const lex: LexDefinitions = [
	{token:&amp;quot;DIGITS&amp;quot;, pattern:/[1-9][0-9]*/},
	{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, pattern:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, pattern:&amp;quot;(&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, pattern:&amp;quot;)&amp;quot;},
	{token:null, pattern:/\s/},
	{token:&amp;quot;INVALID&amp;quot;, pattern:/./},
];
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この字句規則の定義について、実用上の理由で追加したいくつかの仕様に注意する必要があります。&lt;/p&gt;

&lt;p&gt;Tokenの型定義にsymbolを含めている点についてはここで説明せずに後述することとします。&lt;/p&gt;

&lt;h3 id=&#34;入力の読み捨て:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;入力の読み捨て&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;{token:null, pattern:/\s/}
// (読み捨て): /\s/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この部分は、何らかの空白文字が入力に存在していればマッチングされます。
&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; を解析する際、ここに含まれている空白は文法上何の意味も持たず、&lt;code&gt;9+11*(2+1)&lt;/code&gt; のように入力が与えられたとしても解析結果は変化しません。
このような場合、構文解析器に空白の情報を与えることすらせずに、字句解析器上で空白を検知した段階でその情報を捨ててしまったほうが、構文解析器に余計な処理をさせずに済みます。&lt;/p&gt;

&lt;p&gt;今回は、トークンのラベル部分にnullを指定することで、読み取った結果をトークンとして保持することなく読み捨てることを表すようにしています。&lt;/p&gt;

&lt;h3 id=&#34;正規表現パターンと文字列パターンの使い分け:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;正規表現パターンと文字列パターンの使い分け&lt;/h3&gt;

&lt;p&gt;(ごちゃごちゃ書いている割に小手先のテクニックという感じが強いため、よくわからなければ読み飛ばしてください)&lt;/p&gt;

&lt;p&gt;&lt;code&gt;{token:&amp;quot;PLUS&amp;quot;, pattern:&amp;quot;+&amp;quot;}&lt;/code&gt; のように、パターン部分に正規表現ではなく文字列を用いて記述している箇所があります。
すべて正規表現を使って記述するのではなく文字列も許容している理由として、まず&lt;code&gt;/\+/&lt;/code&gt;のように特殊記号をエスケープせずに済む点が挙げられます。
そして、「文字列でパターンを記述した場合は、アルファベットの途中でトークンを区切らないようにする」というルールを用いることで、一部のパターンを簡潔に書くことが可能になります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;vwxyz&lt;/code&gt; という入力を考えてみましょう。
ここでもし、&lt;code&gt;&amp;quot;vwx&amp;quot;&lt;/code&gt; にマッチする規則と、&lt;code&gt;&amp;quot;vwxyz&amp;quot;&lt;/code&gt; にマッチする規則の2つが存在した場合、&lt;code&gt;&amp;quot;vwxyz&amp;quot;&lt;/code&gt; の規則を先に書かない限り、入力&lt;code&gt;vwxyz&lt;/code&gt; は&lt;code&gt;&amp;quot;vwx&amp;quot; + yz&lt;/code&gt; とみなされ、&lt;code&gt;&amp;quot;yz&amp;quot;&lt;/code&gt; に対応する規則が存在しなければエラーとなります。
これを回避するためには、よりマッチするパターンが長い規則を常に短い規則よりも先に書くようにする必要がありますが、面倒です。
そこで、正規表現ではなく文字列で&lt;code&gt;&amp;quot;vwx&amp;quot;&lt;/code&gt; などのパターンが定義され、かつその末尾の文字が&lt;code&gt;\w&lt;/code&gt; にマッチする場合、マッチした部分の一文字先の文字が&lt;code&gt;\w&lt;/code&gt; 以外でなければマッチしないようにします。
これは、正規表現で&lt;code&gt;/vwx(?!\w)/&lt;/code&gt; 、&lt;code&gt;/vwxyz(?!\w)/&lt;/code&gt; というような否定的前方先読みをパターンの最後に追加することに相当します。
このルールを追加することで、正規表現を用いる場合よりも簡潔に記述可能となります。&lt;/p&gt;

&lt;h2 id=&#34;字句解析器を実装する:33ced0e1bdc35ac00ed380cf2d7265fa&#34;&gt;字句解析器を実装する&lt;/h2&gt;

&lt;p&gt;ではLexerクラスを作っていきましょう。
とはいえ字句規則さえ定義してしまえば、やることはほとんどありません。
コンストラクタ引数として字句規則データを受け取って保持しておくようにして、解析実行時に上から順に字句規則のマッチングを試みるだけです。&lt;/p&gt;

&lt;p&gt;今回はコード量が少ないので、 &lt;a href=&#34;https://github.com/Tatamo/parsergenerator/blob/master/src/lexer.ts&#34;&gt;https://github.com/Tatamo/parsergenerator/blob/master/src/lexer.ts&lt;/a&gt; 全体をそのまま貼り付けます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// token.ts
export type Token = string|symbol;
export const SYMBOL_EOF:Token = Symbol(&amp;quot;EOF&amp;quot;);
export const SYMBOL_SYNTAX:Token = Symbol(&amp;quot;S&#39;&amp;quot;);
export const SYMBOL_DOT:Token = Symbol(&amp;quot;.&amp;quot;);

export type TokenList = Array&amp;lt;{token:Token, value:string}&amp;gt;;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;/// lexer.ts
/// LexDefinitionsの定義は先述のものと同一
import {Token, SYMBOL_EOF, TokenList} from &amp;quot;./token&amp;quot;;
import {LexDefinitions} from &amp;quot;./grammar&amp;quot;;

export interface ILexer{
	exec(str: string):TokenList;
}

export class Lexer implements ILexer{
	constructor(public def: LexDefinitions){
		// 正しいトークン定義が与えられているかチェック
		for(var i=0; i&amp;lt;this.def.length; i++){
			var token_pattern = this.def[i].pattern;
			if(typeof token_pattern == &amp;quot;string&amp;quot;){
				continue;
			}
			else if(token_pattern instanceof RegExp){
				// フラグを整形する
				let flags:string = &amp;quot;&amp;quot;;
				// gフラグは邪魔なので取り除く
				// i,m,uフラグがあれば維持する
				if(token_pattern.ignoreCase){
					flags += &amp;quot;i&amp;quot;;
				}
				if(token_pattern.multiline){
					flags += &amp;quot;m&amp;quot;;
				}
				if(token_pattern.unicode){
					flags += &amp;quot;u&amp;quot;;
				}
				// yフラグは必ずつける
				flags += &amp;quot;y&amp;quot;;
				// フラグをつけなおして新しい正規表現オブジェクトにする
				this.def[i].pattern = new RegExp(token_pattern, flags);
				continue;
			}
			throw new Error(&amp;quot;invalid token definition: neither string nor RegExp object&amp;quot;);
		}
	}
	exec(str: string):TokenList{
		var result:TokenList = [];
		let lastindex = 0;
		while(lastindex &amp;lt; str.length){
			for(var i=0; i&amp;lt;this.def.length; i++){
				var token:Token|null = this.def[i].token;
				var token_pattern = this.def[i].pattern;
				var match:string;
				if(typeof token_pattern == &amp;quot;string&amp;quot;){
					let last_tmp = lastindex+token_pattern.length;
					if(str.substring(lastindex,last_tmp) != token_pattern) continue;
					if(last_tmp &amp;lt; str.length &amp;amp;&amp;amp; /\w/.test(token_pattern.slice(-1)) &amp;amp;&amp;amp; /\w/.test(str[last_tmp])) continue; // ヒットした文字の末尾が\wで、そのすぐ後ろが\wの場合はスキップ
					match = token_pattern;
					lastindex += token_pattern.length;
				}
				else{
					// token_pattern: RegExp
					token_pattern.lastIndex = lastindex;
					let m = token_pattern.exec(str);
					if(m === null) continue; // マッチ失敗
					match = m[0];
					lastindex = token_pattern.lastIndex; // lastindexを進める
				}
				// tokenがnullなら処理を飛ばします
				if(token != null){
					result.push({token:token, value:match});
				}
				break;
			}
		}
		// 最後にEOFトークンを付与
		result.push({token:SYMBOL_EOF, value:&amp;quot;&amp;quot;});
		return result;
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まずコンストラクタでは、与えられた字句規則に簡単な型チェックと正規表現の整形を行っています。
グローバルマッチは今回は邪魔なだけなので、与えられた正規表現にgフラグがついていれば取り除きます。
ES2015でRegExpに追加されたstickyフラグ(&lt;strong&gt;ほぼ&lt;/strong&gt;全ての主要モダンブラウザ上で実装済み)を使うと楽なので、ここでyフラグの追加も行います。&lt;/p&gt;

&lt;p&gt;execメソッドでは入力を読み終えるまでマッチングを繰り返し、&lt;code&gt;{token:Token, value:string}&lt;/code&gt; というオブジェクトを結果の配列に追加していきます。
先述のようにパターンが文字列であれば&lt;code&gt;\w&lt;/code&gt;が連続した場所では区切らないようにして、マッチングが成功するたびにインデックス位置を先に進めていきます。&lt;/p&gt;

&lt;p&gt;また、すべての入力を読み終えた後、最後にSymbol(EOF)を名前としたトークンを結果に追加します。
これは入力の末尾を意味するトークンで、構文解析の際に内部的に使用されます。&lt;/p&gt;

&lt;p&gt;(Symbolは、それ自身と比較しない限り&lt;code&gt;==&lt;/code&gt;や&lt;code&gt;===&lt;/code&gt;の評価結果が常にfalseになるプリミティブ型で、ES2015で追加されたものです。
字句規則で定義されたトークンとの衝突が発生しないようにここでSymbolを使用していますが、Symbolそのものはオブジェクトのプロパティとして使用することで後方互換性を維持することを目的としてJavaScriptに追加された型であるため、この用途で用いるのに適しているのかどうかは議論の余地があります。
とはいえプログラミング言語個別の問題はこの記事の主題とは関係がないため、詳しくは言及しません。)&lt;/p&gt;

&lt;p&gt;この字句解析器に先ほどの字句規則を与え、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; を入力すると、&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-TypeScript&#34;&gt;[
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;9&amp;quot;},
	{token:&amp;quot;PLUS&amp;quot;, value:&amp;quot;+&amp;quot;},
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;11&amp;quot;},
	{token:&amp;quot;ASTERISK&amp;quot;, value:&amp;quot;*&amp;quot;},
	{token:&amp;quot;LPAREN&amp;quot;, value:&amp;quot;(&amp;quot;},
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;2&amp;quot;},
	{token:&amp;quot;PLUS&amp;quot;, value:&amp;quot;+&amp;quot;},
	{token:&amp;quot;DIGITS&amp;quot;, value:&amp;quot;1&amp;quot;},
	{token:&amp;quot;RPAREN&amp;quot;, value:&amp;quot;)&amp;quot;},
	{token:Symbol(EOF), value:&amp;quot;&amp;quot;}
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;という結果が得られます。
これでトークンの並びを得ることに成功したので、次回以降はいよいよパーサジェネレータの作成に移っていくことになります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/&#34;&gt;前回:かんたん構文解析入門&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;http://tatamo.81.la/blog/2017/03/21/lr-parser-generator-implementation-03/&#34;&gt;次回:儀式の下準備&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>LR(1)パーサジェネレータを自作して構文解析をする 第1回:かんたん構文解析入門</title>
      <link>http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/</link>
      <pubDate>Thu, 22 Dec 2016 03:03:09 +0900</pubDate>
      
      <guid>http://tatamo.81.la/blog/2016/12/22/lr-parser-generator-implementation/</guid>
      <description>
        

&lt;p&gt;この記事は&lt;a href=&#34;http://www.adventar.org/calendars/1881&#34;&gt;Kobe University Advent Calendar 2016&lt;/a&gt;の21日の記事です。また遅刻か。
なお私は当該大学の学部2年(2016年12月現在)です。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;構文解析ができるプログラマはちょっとかっこいいですよね。
「構文解析？ああ、できますよ」とか言って自分のスキルを自慢できそうな印象があります。&lt;/p&gt;

&lt;p&gt;(ほぼ)フルスクラッチでTypeScriptによるLR(1)パーサジェネレータを実装した(ついでにLALR(1)パーサも作れる)ので、これを完成させるまでの流れを紹介していこうと思います。&lt;/p&gt;

&lt;p&gt;今回は構文解析自体の入門編となります。&lt;/p&gt;

&lt;p&gt;自作したパーサジェネレータは &lt;a href=&#34;https://github.com/Tatamo/parsergenerator&#34;&gt;https://github.com/Tatamo/parsergenerator&lt;/a&gt; にあります。&lt;br /&gt;
今のところパーサジェネレータ部分は完成、基本的な構文解析なら問題なくこなせるので構文規則や字句規則を外部から読み取って構文解析してパーサジェネレータに渡すような処理や、全体の見通しを良くするための設計の見直しやリファクタリング等を行っている段階です。
ドキュメント作ってなくてすみません。&lt;/p&gt;

&lt;h2 id=&#34;構文解析をしたい:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析をしたい&lt;/h2&gt;

&lt;p&gt;構文解析、時々見かけるフレーズです。
プログラマなら覚えておいて損はない技術……かどうかはわかりませんが、そういう類のスキルに(傍からは)見えます。&lt;br /&gt;
ぜひやりましょう。&lt;/p&gt;

&lt;p&gt;ひとまず、何をやりたいのかを明確にする必要があります。&lt;br /&gt;
この記事では、「入力として与えられるLR(1)文法に属する文法に従ったトークン列をパース(構文解析)することで、その構造を構文木として出力する」ことを目標とします。
何を言っているのかさっぱりわかりませんね、わからなくていいです。&lt;/p&gt;

&lt;p&gt;順を追って説明する必要がありますが、詳細は適宜省略します。
そのため、まずは今回主に参照した資料を列挙しておきます。&lt;/p&gt;

&lt;h2 id=&#34;参考資料一覧:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;参考資料一覧&lt;/h2&gt;

&lt;p&gt;より詳しく知りたい方は、下記に挙げる資料やそこで紹介されている参考文献などを参照されるのが良いと思われます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.cornell.edu/courses/cs412/2003sp/lectures/lec09.pdf&#34;&gt;Cornell CIS Introduction to Compilers Lecture 9: LR(1) Parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jaist.ac.jp/~kshirai/lec/i223/04a.pdf&#34;&gt;JAIST 自然言語処理論Ｉ 4.文法2(構文解析) その1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.jaist.ac.jp/~kshirai/lec/i223/04b.pdf&#34;&gt;JAIST 自然言語処理論Ｉ 4.文法2(構文解析) その2&lt;/a&gt;
(注：「LR法による構文解析」として紹介されているアルゴリズムはSLR法)&lt;br /&gt;
上記3つはネット上にアップロードされている特定の大学の講義資料ですが、公開の規定等を確認していないためリンクを張ることに不都合があるようなら知らせていただけると助かります。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Canonical_LR_parser&#34;&gt;Canonical LR parser - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/ichikaz3/lr-parsing&#34;&gt;LR parsing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/uhyo_&#34;&gt;うひょ(@uhyo_)さん&lt;/a&gt; 生き字引。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;構文解析とは-ざっくり:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析とは？(ざっくり)&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; という数式を考えてみます。
構文解析をすることによる最終的な目的は、この数式を(たとえば)文字列として与えると、結果としてこの数式の答えが&lt;code&gt;42&lt;/code&gt;であることを導く、といったことです。&lt;/p&gt;

&lt;p&gt;そのためには、以下のものが必要になります：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数式を表現する構文規則&lt;/li&gt;
&lt;li&gt;上記構文規則を解析するように作られた構文解析器(Parser)&lt;/li&gt;
&lt;li&gt;解析された構文を処理するプログラム&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;さらに、これらの構文解析に入る前の下準備のために以下が必要です：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;文字列をトークンとして分割して表現するための字句規則&lt;/li&gt;
&lt;li&gt;上記字句規則をもとに、文字列を読み取ってトークンを返す字句解析器(Lexical Analyzer、略してLexer)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ちなみに、今回の記事の目標は、それらに加えて以下のものを実装することです：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;構文規則および字句規則を入力として与えることで、構文解析器そのものを自動生成するパーサジェネレータ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;実際の構文解析を行う手順とはずれてしまいますが、紹介した順番に沿って構文解析器→字句解析器の順に解説していきます。&lt;/p&gt;

&lt;h3 id=&#34;構文解析器-パーサ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;構文解析器(パーサ)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; という数式を解析するためには、まずこの数式がどのようなルールで記述されているのかを(再)定義する必要があります。
そのルールをを表すのが構文規則です。
構文規則を書き表すルールは、たとえば&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AB%E3%82%B9%E3%83%BB%E3%83%8A%E3%82%A6%E3%82%A2%E8%A8%98%E6%B3%95&#34;&gt;BNF&lt;/a&gt;など様々な種類がありますが、基本的な発想としては&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; X Y Z
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように左辺の記号を右辺の記号の並びによって定義することで行います。&lt;/p&gt;

&lt;p&gt;具体的に見てみましょう。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;式 -&amp;gt; 式 &amp;quot;+&amp;quot; 項
式 -&amp;gt; 項
項 -&amp;gt; 項 &amp;quot;*&amp;quot; 因子
項 -&amp;gt; 因子
因子 -&amp;gt; 数
因子 -&amp;gt; &amp;quot;(&amp;quot; 式 &amp;quot;)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;妥当ですね。
&lt;code&gt;式 -&amp;gt; 式 + 項&lt;/code&gt; と &lt;code&gt;式 -&amp;gt; 項&lt;/code&gt;の２つの規則が、再帰的な繰り返しを表現していることに注意してください。
たとえば、&lt;code&gt;項&lt;/code&gt;は当然&lt;code&gt;式&lt;/code&gt;ですし、&lt;code&gt;項 + 項&lt;/code&gt;も&lt;code&gt;式(-&amp;gt;項) + 項&lt;/code&gt; より&lt;code&gt;式&lt;/code&gt;となります。
さらに、&lt;code&gt;項 + 項 + 項&lt;/code&gt;は最初の&lt;code&gt;項 + 項&lt;/code&gt;が&lt;code&gt;式&lt;/code&gt;なので、&lt;code&gt;式(-&amp;gt;項 + 項) + 項&lt;/code&gt; より&lt;code&gt;式&lt;/code&gt;です。
よって、&lt;code&gt;式&lt;/code&gt;は&lt;code&gt;項&lt;/code&gt;を&lt;code&gt;&amp;quot;+&amp;quot;&lt;/code&gt;によって任意の回数だけ繋げたものであり、同様に&lt;code&gt;項&lt;/code&gt;は&lt;code&gt;因子&lt;/code&gt;を&lt;code&gt;&amp;quot;*&amp;quot;&lt;/code&gt;で繋げたものとなります。
最後に、&lt;code&gt;因子&lt;/code&gt;は単なる&lt;code&gt;数&lt;/code&gt;かもしれませんし、または&lt;code&gt;&amp;quot;(&amp;quot;&lt;/code&gt;と&lt;code&gt;&amp;quot;)&amp;quot;&lt;/code&gt;で囲まれた&lt;code&gt;式&lt;/code&gt;かもしれません。
これは括弧で囲まれた部分の式が他の部分よりも高い優先順位となることを表現しています。&lt;/p&gt;

&lt;p&gt;たとえば&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt;は、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;式{ [9] [+] [11 * (2 + 1)] }
式{ 項{ [9] } &amp;quot;+&amp;quot; 項{ [11] [*] [(2 + 1)] } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ [(] [2 + 1] [)] } } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ [2] [+] [1] } &amp;quot;)&amp;quot; } } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ 項{ [2] } &amp;quot;+&amp;quot; 項{ [1] } } &amp;quot;) &amp;quot;} } }
式{ 項{ 因子{9} } &amp;quot;+&amp;quot; 項{ 因子{11} &amp;quot;*&amp;quot; 因子{ &amp;quot;(&amp;quot; 式{ 項{ 因子{2} } &amp;quot;+&amp;quot; 項{ 因子{1} } } &amp;quot;)&amp;quot; } } }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように展開されます(こうして得られた構造をどう解析するかについては省略します)。この、解析対象→構文木の変換を自動で行うのがパーサです。&lt;/p&gt;

&lt;p&gt;ちなみにですが、この構文規則は解析したい対象ごとにあなたが一から書き上げる必要があります。&lt;/p&gt;

&lt;h3 id=&#34;字句解析器-レキシカルアナライザ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;字句解析器(レキシカルアナライザ)&lt;/h3&gt;

&lt;p&gt;上記構文規則では、&lt;code&gt;+&lt;/code&gt;や&lt;code&gt;*&lt;/code&gt;のような演算子、&lt;code&gt;数&lt;/code&gt;についての規定はありません。
これらの「左辺に現れない記号」を、「終端記号」と呼びます。左辺に現れる記号は非終端記号と呼ばれます。&lt;/p&gt;

&lt;p&gt;通常、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt; のような入力は文字列で与えられますが、記号と記号の間には複数もしくは0個の空白が挿入されている可能性もあります。
しかし以下のような構文規則を定義するのは本質的ではありません：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;空白 -&amp;gt; &amp;quot; &amp;quot;
空白 -&amp;gt; &amp;quot; &amp;quot; 空白
数字 -&amp;gt; &amp;quot;0&amp;quot; | &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | ... | &amp;quot;9&amp;quot;
数字 -&amp;gt; (&amp;quot;0&amp;quot; | &amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | ... | &amp;quot;9&amp;quot;) 数字
数 -&amp;gt; (&amp;quot;1&amp;quot; | &amp;quot;2&amp;quot; | &amp;quot;3&amp;quot; ... | &amp;quot;9&amp;quot;) 数字
ただし、|は「または」を表す
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;そこで、通常は「入力として与えられた文字列」を「終端記号として分類されたトークンの列」に変換する処理をはさみ、これによって得られたトークンを構文解析器に与えます。
トークンとは終端記号と、必要ならばそれに紐付いた元々の情報を保持しておいたものです。たとえば、&lt;code&gt;9 + 11 * (2 + 1)&lt;/code&gt;は、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: 9
プラス: +
数字: 11
アステリスク: *
左括弧: (
数字: 2
プラス: +
数字: 1
右括弧: )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というような9つのトークンの列に分けることができます。
構文解析器はそのトークンがどのような終端記号に対応しているかは見ますが、たとえば個々の数字が何であるかを判断することはしません。
これによって、構文解析器は本質的な文法の解析のみに注力することができます。&lt;/p&gt;

&lt;p&gt;この処理をするのが字句解析器で、どのような文字や文字列が与えられた場合に何という終端記号かを判別するための規則が字句規則です。&lt;/p&gt;

&lt;p&gt;字句規則は、例えば以下のような書き方になるでしょう：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;数字: /[1-9][0-9]*/
プラス: &amp;quot;+&amp;quot;
アステリスク: &amp;quot;*&amp;quot;
左括弧: &amp;quot;(&amp;quot;
右括弧: &amp;quot;)&amp;quot;
(読み捨て): /\s/
(不正): /./
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここでは字句規則の表現のために、文字列および正規表現を使用しています。
通常(?)字句規則は上から順に文字列の先頭部分を当てはめていき、マッチするものがあればその終端記号に対応付けます。
そのため、&lt;code&gt;(不正)&lt;/code&gt;の部分は入力された文字全てにマッチする正規表現&lt;code&gt;/./&lt;/code&gt;が使用されていますが、これは上の規則のいずれにも当てはまらなかった場合にのみマッチします。&lt;/p&gt;

&lt;p&gt;与えられた文字列を前から順番に見ていくだけなので、字句解析器の実装はパーサやパーサジェネレータの実装と比べると単純です。&lt;/p&gt;

&lt;h3 id=&#34;パーサジェネレータ:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;パーサジェネレータ&lt;/h3&gt;

&lt;p&gt;ここまで構文解析器(パーサ)と字句解析器(レキシカルアナライザ)について見てきました。
基本的にはこの2つによって構文解析を行うことができ、基本的な流れとしては&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;入力となるような解析したい言語を用意する&lt;/li&gt;
&lt;li&gt;字句規則を用意して、それをもとにしたレキシカルアナライザを用意する&lt;/li&gt;
&lt;li&gt;レキシカルアナライザに入力を与え、トークンの列を取得する&lt;/li&gt;
&lt;li&gt;構文規則を用意して、それをもとにしたパーサを用意する&lt;/li&gt;
&lt;li&gt;パーサにトークンの列を与え、解析結果を得る&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;となります。
パーサジェネレータとは何かというと、この 4. の部分を自動化するものです。
(LR法の)構文解析器は、内部的には入力を受け取ってスタックに積みながら状態遷移を繰り返すオートマトンにすぎません。
そのため、どの入力が与えられればどのような状態に遷移するかを示す「構文解析表」を得ることができれば、その構文を解析するパーサを作成することができます。
パーサジェネレータは、構文規則を読み取ることでこの構文解析表をつくり上げるという処理を主に行います。&lt;/p&gt;

&lt;p&gt;字句解析器程度ならわざわざジェネレータを作らなくても、字句規則そのものを字句解析器に渡せば良い感じに字句解析してくれるようにできますが、パーサジェネレータも「構文解析表の構築後、それをもとにして構文解析を行う」ような機能がついていればそれはパーサであるとも言えます。
わざわざパーサとパーサジェネレータが分けられているのは、一つには計算資源の乏しかった昔はパーサジェネレータがオンメモリで展開した構文解析表をもとにそのままパーサとして振る舞うというようなことが少なく、構文解析表を与えることで「パーサのソースコード」を出力するようなものが一般的だったからではないかと思われます(適当な思いつきを言っています)。
もっとも、パーサジェネレータがパーサを生成する際の処理にかかる時間を省略したい場合、予めパーサをコンパイルしておけるようにするのは妥当といえるでしょう。
字句解析器のための「字句解析器ジェネレータ」も実際に存在していますが、ここでは簡単のために字句解析器はコンストラクタに字句規則を与えれば勝手に良い感じの字句解析を行ってくれるようになるものと思ってもらえればよいです。&lt;/p&gt;

&lt;h3 id=&#34;文脈自由言語について:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;文脈自由言語について&lt;/h3&gt;

&lt;p&gt;構文解析器が解析対象とする「言語」がどのようなものであるかについてはいろいろな定義がなされています。&lt;/p&gt;

&lt;p&gt;これについては、参考資料でも紹介した&lt;a href=&#34;https://twitter.com/ki6o4&#34;&gt;うさぎさん(@ki6o4)&lt;/a&gt;の&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kimiyuki.net/blog/2016/08/03/context-free-grammar/&#34;&gt;文脈自由文法とその構文解析法 &amp;middot; うさぎ小屋&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が詳しいため、こちらを参照していただくことをおすすめします。
ここでは、厳密な話はあまりせずにごくごく簡単に触れていこうと思います。&lt;/p&gt;

&lt;p&gt;構文解析の対象とするのは、基本的に文脈自由言語となります。
構文解析の手法にも様々なものがありますが、それらの手法の中には文脈自由言語すべてを解析できるわけではないものも多く、たとえばLR(1)法ならLR(1)文法やLR(1)言語というように、ある手法で解析できる文法や、解析できる言語全体をその手法の名前で表される言語として表現することがあります。&lt;/p&gt;

&lt;h3 id=&#34;解析手法と言語のクラス:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;解析手法と言語のクラス&lt;/h3&gt;

&lt;p&gt;いくつかの手法を主観を交えて乱暴に紹介していきます。&lt;/p&gt;

&lt;h4 id=&#34;先読み:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;先読み&lt;/h4&gt;

&lt;p&gt;**この項は下のLR法などの項を「先読み」してから戻ってきて読むことをおすすめします**&lt;/p&gt;

&lt;p&gt;たとえばLR(1)法のように、数字を括弧でくくって(k)と表現している手法がいくつかあります。このkは何文字先読みするかを示していて、たとえば(1)ならば1文字先読みするという意味です。
先読み数については、たとえばLR法については以下のようなことが言われています。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;LR(k)で表せる文法のクラス ⊆ LR(k+1)先読みで表せるクラス である&lt;/li&gt;
&lt;li&gt;LR(k)文法によって受理可能な言語のクラスは、LR(1)のそれと等しい&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2.より、基本的には(1)について考えることが多いようです。&lt;/p&gt;

&lt;h4 id=&#34;ll-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LL(1)法&lt;/h4&gt;

&lt;p&gt;「再帰を使って構文解析する」という発想としては単純なもの。
LL(1)文法のクラスはLR(1)よりも大幅に小さいものの、それでもLALR(1)文法を外れた文法を解析できたりします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; S + E
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;というような、右辺の一番左の場所に左辺の記号が登場するような「左再帰則」を読むことができません。ナンセンス。&lt;/p&gt;

&lt;h4 id=&#34;lr-0-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(0)法&lt;/h4&gt;

&lt;p&gt;先読み数が0なのでよわい。&lt;/p&gt;

&lt;h4 id=&#34;slr法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;SLR法&lt;/h4&gt;

&lt;p&gt;SLRのSはSimpleの意味です。LR(0)から単純な先読みを加えることでLR(0)よりも解析可能な文法が増えますが、それでもLALR(1)には及びません。&lt;/p&gt;

&lt;h4 id=&#34;lr-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(1)法&lt;/h4&gt;

&lt;p&gt;LR(0)に対し、1文字だけ先読みして次にどのような入力が期待されるかを判断。
LR(1)文法がそれなりに広いという点で優秀な一方、LR(0)に比べて構文解析表の大きさが爆発しやすいという欠点がある、と言われています。
しかし今の時代はそんなものは大した欠点になり得ない気がします。&lt;/p&gt;

&lt;h4 id=&#34;lalr-1-法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LALR(1)法&lt;/h4&gt;

&lt;p&gt;プログラミング言語を解析するコンパイラなどによく使われている手法です。
LALR(1)のLAはLook-Aheadの略で、まずLR(1)法で構文解析表を作ってから、文法部分が同じで先読み記号だけが違うような状態をマージするという点がLR(1)法と異なります。
表を併合してしまうためにLR(1)法よりも解析可能な文法のクラスが小さくなるものの、実用上はほとんど問題にならず、LR(1)法の構文解析表が大きくなりすぎるという欠点を補える手法です。&lt;/p&gt;

&lt;p&gt;ただし、一度LR(1)法の表を作ること変わりはないのでメモリ消費量はそう変わらないし、大きなデータも問題なく扱える今の時代にわざわざ構文解析表を数十パーセント程度削減したところで何の意味があるのかという疑問があります。&lt;/p&gt;

&lt;p&gt;また、LALR法のLAはLook-Aheadの略だと言いましたが、注意しなければならないのは&lt;strong&gt;Look-Ahead(先読み)を行うのはLALR法固有の手法ではない&lt;/strong&gt;ということです。  先読み自体はLR(1)法でもやりますし、LALR(1)はあくまでLR(1)の先読み部分をマージしたものにすぎません。
私はLALR法の名前の付け方はあまり良くないと思っていて、MLR法(Merged Look-Ahead LR法)とかなんとか、そういう感じの名前に変えたほうが良いと思います。&lt;/p&gt;

&lt;h4 id=&#34;glr法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;GLR法&lt;/h4&gt;

&lt;p&gt;「あいまいな」解釈が可能な文法があった場合、考えられうるすべての可能性を探索してしまうことによって解決する手法。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;「お魚くわえた猫を追いかけるサザエさん」&lt;/code&gt;で魚をくわえているのが猫とサザエさんの両方に解釈できるように、一つの入力に対して複数の結果が得られることがあります。
どちらかというと自然言語処理向きかもしれません。&lt;/p&gt;

&lt;h4 id=&#34;cyk法:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;CYK法&lt;/h4&gt;

&lt;p&gt;強力なアルゴリズムにより、文脈自由言語すべてを比較的高速に解析可能。
ただし、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;S -&amp;gt; NP VP
VP -&amp;gt; v
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように、チョムスキー標準形といわれるような、「右辺が非終端記号ちょうど2つか終端記号1つでなければならない」、つまり結果として得られる構造が二分木になっていなければならないというナンセンスにも程がある制約を課されます(アルゴリズムの改良や規則の変換、得られた木構造の後処理などによって回避は可能ですが)。&lt;/p&gt;

&lt;p&gt;このCYK法の計算量オーダーは&lt;code&gt;O(n^3)&lt;/code&gt;程度で、文脈自由言語全てを解析可能なアルゴリズムの中では高速ですが、この記事で紹介されている他のアルゴリズムよりは低速となります。
たとえばLR(1)法は文脈自由言語全体を解析出来ないかわりに&lt;code&gt;O(n)&lt;/code&gt;で解析が可能です。&lt;/p&gt;

&lt;p&gt;プログラミング言語の解析では、言語の開発者が文法自体をある程度自由に定義することができるため、文脈自由言語の一部だけでなく全体を解析したいという需要はあまり発生しません。&lt;/p&gt;

&lt;h2 id=&#34;lr-1-パーサジェネレータをつくろう:4f3ac79f73d5f2d4c7087437a39bd22b&#34;&gt;LR(1)パーサジェネレータをつくろう&lt;/h2&gt;

&lt;p&gt;構文解析の大まかな流れはわかりました。
とりあえず字句規則と構文規則を用意して、あとはどうにかしてこの構文を読んでくれるようなパーサを用意すれば構文解析ができそうです(字句解析器なんてのは適当にやってもすぐ用意できます)。&lt;/p&gt;

&lt;p&gt;先ほど紹介したような手法によって構文ごとに一からパーサをプログラミングするようなことはやりたくないので、パーサジェネレータを用いてパーサを自動的に生成してもらえば事は済みそうですね。&lt;/p&gt;

&lt;p&gt;ここに&lt;a href=&#34;https://www.gnu.org/software/bison/&#34;&gt;Bison&lt;/a&gt;という有名なパーサジェネレータがあります。
今の時代にわざわざCやC++で構文解析なんてしたくないのでしたら、Pythonで&lt;a href=&#34;http://www.dabeaz.com/ply/&#34;&gt;PLY&lt;/a&gt;とか、JavaScriptの&lt;a href=&#34;https://github.com/zaach/jison&#34;&gt;jison&lt;/a&gt;というものなど、いくらでも選択肢があります。
これらのうち一つを選んで、チュートリアルを読んでパーサを作っていくのがいいでしょう。&lt;/p&gt;

&lt;p&gt;というわけで、前段を書いていると結構分量が膨らんでしまったため、今回はここで区切ります。&lt;/p&gt;

&lt;p&gt;では次回からは、構文解析を行えるようになるため、LR(1)法を用いたパーサジェネレータを実際に作っていく流れを紹介していきたいと思います。&lt;/p&gt;

&lt;p&gt;えっちょっとまって、今パーサジェネレータは既存のものを使えばいいって言ったよね、ねえ&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://tatamo.81.la/blog/2017/02/11/lr-parser-generator-implementation-02/&#34;&gt;次回:字句解析器の実装&lt;/a&gt;&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>